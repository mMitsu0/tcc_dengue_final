{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mMitsu0/tcc_dengue_final/blob/main/dengue_previs_o.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **CENTRO UNIVERSITÁRIO NOSSA SENHORA DO PATROCÍNIO (CEUNSP)**\n",
        "SALTO - SP\n",
        "\n",
        "**BARACHELADO EM CIÊNCIA DA COMPUTAÇÕES**\n",
        "\n",
        "# ***Análise Comparativa de Algoritmos de Machine Learning para Predição de Casos de Dengue.***\n",
        "\n",
        "Autores:\n",
        "*Matheus Mitsuo Tomotake Santos - RGM: 30248531*\n",
        "*Pedro Gavioli Pinarde - RGM: 25798006*\n",
        "*Lucas Felix Nogueira -RGM: 28851994*\n",
        "*Gabriel Souza Cavalcante: RGM - 30386730*"
      ],
      "metadata": {
        "id": "LQWrz75kd3Vj"
      },
      "id": "LQWrz75kd3Vj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORT DAS BIBLIOTECAS NECESSÁRIAS**"
      ],
      "metadata": {
        "id": "mEtsxlxtTdCy"
      },
      "id": "mEtsxlxtTdCy"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "he2k711kQHh6"
      },
      "outputs": [],
      "source": [
        "# === Utilidades do sistema e dados ===\n",
        "import os  # Interações com o sistema operacional (ex: manipulação de arquivos e diretórios)\n",
        "import pandas as pd  # Manipulação e análise de dados em tabelas (DataFrames)\n",
        "import numpy as np  # Operações matemáticas e numéricas eficientes com arrays\n",
        "\n",
        "# === Visualização de dados ===\n",
        "import matplotlib.pyplot as plt  # Criação de gráficos e visualizações personalizadas\n",
        "import seaborn as sns  # Visualizações estatísticas com design aprimorado\n",
        "\n",
        "# === Requisições e manipulação de arquivos ===\n",
        "import requests  # Requisições HTTP para acessar APIs e baixar dados\n",
        "from io import StringIO  # Permite tratar strings como arquivos (útil para ler CSVs diretamente da web)\n",
        "\n",
        "# === Machine Learning Clássico ===\n",
        "from sklearn.model_selection import train_test_split  # Divisão dos dados em conjuntos de treino e teste\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor  # Modelos baseados em árvores de decisão\n",
        "from sklearn.linear_model import LinearRegression  # Modelo de regressão linear tradicional\n",
        "from xgboost import XGBRegressor  # Modelo de regressão baseado em boosting com alta performance\n",
        "from sklearn.metrics import mean_squared_error, r2_score  # Métricas de avaliação (MSE e R²)\n",
        "from sklearn.preprocessing import StandardScaler  # Normalização e padronização de variáveis (features)\n",
        "\n",
        "# === Deep Learning (LSTM) ===\n",
        "import tensorflow as tf  # Biblioteca de aprendizado profundo\n",
        "from tensorflow.keras import layers, models, callbacks, optimizers  # Componentes do Keras para redes neurais LSTM\n",
        "\n",
        "# === Configuração de ambiente (para Jupyter) ===\n",
        "try:\n",
        "    get_ipython().run_line_magic(\"matplotlib\", \"inline\")  # Exibe gráficos diretamente no notebook\n",
        "except:\n",
        "    pass  # Ignora se não estiver em ambiente Jupyter\n"
      ],
      "id": "he2k711kQHh6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CONEXÃO COM A API E DOWNLOAD DOS DADOS**"
      ],
      "metadata": {
        "id": "2ScdGc2iTvOD"
      },
      "id": "2ScdGc2iTvOD"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Uv7bslxSQHh7"
      },
      "outputs": [],
      "source": [
        "# === IMPORTS ===\n",
        "import os  # Interações com o sistema operacional (ex: criar pastas)\n",
        "import requests  # Requisições HTTP (para baixar dados da API InfoDengue)\n",
        "import pandas as pd  # Manipulação e análise de dados tabulares\n",
        "from io import StringIO  # Permite tratar strings como arquivos (útil para ler CSV direto da API)\n",
        "from datetime import datetime  # Controle de datas (para definir ano atual automaticamente)\n",
        "\n",
        "# ===============================================================\n",
        "# === 1. DOWNLOAD DOS DADOS ===\n",
        "# ===============================================================\n",
        "\n",
        "# Dicionário com códigos IBGE das capitais brasileiras (usados na API InfoDengue)\n",
        "estados_ibge = {\n",
        "    \"AC\": [1200401], \"AL\": [2704302], \"AM\": [1302603], \"AP\": [1600303],\n",
        "    \"BA\": [2927408], \"CE\": [2304400], \"DF\": [5300108], \"ES\": [3205309],\n",
        "    \"GO\": [5208707], \"MA\": [2111300], \"MG\": [3106200], \"MS\": [5002704],\n",
        "    \"MT\": [5103403], \"PA\": [1501402], \"PB\": [2507507], \"PE\": [2611606],\n",
        "    \"PI\": [2211001], \"PR\": [4106902], \"RJ\": [3304557], \"RN\": [2408102],\n",
        "    \"RO\": [1100205], \"RR\": [1400100], \"RS\": [4314902], \"SC\": [4205407],\n",
        "    \"SE\": [2800308], \"SP\": [3550308], \"TO\": [1721000]\n",
        "}\n",
        "\n",
        "def baixar_dados_por_estado(diretorio_saida=\"data/raw\", ano_inicio=2001, ano_fim=None):\n",
        "    \"\"\"\n",
        "    Função responsável por baixar dados de dengue do InfoDengue\n",
        "    para todas as capitais brasileiras, entre anos definidos.\n",
        "\n",
        "    Parâmetros:\n",
        "    - diretorio_saida (str): caminho onde o CSV final será salvo.\n",
        "    - ano_inicio (int): primeiro ano da série temporal a ser coletada.\n",
        "    - ano_fim (int | None): último ano da série; se None, usa o ano atual.\n",
        "    \"\"\"\n",
        "\n",
        "    # Cria a pasta de destino caso não exista\n",
        "    os.makedirs(diretorio_saida, exist_ok=True)\n",
        "\n",
        "    # URL base da API InfoDengue\n",
        "    base_url = \"https://info.dengue.mat.br/api/alertcity\"\n",
        "\n",
        "    # Lista para armazenar os DataFrames de cada estado\n",
        "    dfs = []\n",
        "\n",
        "    # Define o ano final como o ano atual caso não seja informado\n",
        "    if ano_fim is None:\n",
        "        ano_fim = datetime.now().year\n",
        "\n",
        "    # Loop sobre todos os estados e seus respectivos códigos IBGE\n",
        "    for estado, municipios in estados_ibge.items():\n",
        "        for geocode in municipios:\n",
        "\n",
        "            # Monta os parâmetros da requisição\n",
        "            params = {\n",
        "                \"geocode\": geocode,\n",
        "                \"disease\": \"dengue\",\n",
        "                \"format\": \"csv\",\n",
        "                \"ew_start\": 1,\n",
        "                \"ew_end\": 52,\n",
        "                \"ey_start\": ano_inicio,\n",
        "                \"ey_end\": ano_fim\n",
        "            }\n",
        "\n",
        "            # Constrói a URL final da requisição\n",
        "            url = f\"{base_url}?{'&'.join([f'{k}={v}' for k,v in params.items()])}\"\n",
        "\n",
        "            try:\n",
        "                # Envia requisição HTTP\n",
        "                response = requests.get(url)\n",
        "\n",
        "                # Verifica se a resposta foi bem-sucedida\n",
        "                if response.status_code == 200:\n",
        "                    # Lê o conteúdo CSV direto da resposta e cria um DataFrame\n",
        "                    df = pd.read_csv(StringIO(response.text))\n",
        "                    df[\"estado\"] = estado\n",
        "                    df[\"codigo_ibge\"] = geocode\n",
        "                    dfs.append(df)\n",
        "                    print(f\"✓ Dados coletados para {estado} ({ano_inicio}-{ano_fim})\")\n",
        "                else:\n",
        "                    print(f\" Falha ao coletar {estado} - status {response.status_code}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\" Erro ao processar {estado}: {e}\")\n",
        "\n",
        "    # Após o loop, junta todos os DataFrames e salva em um único CSV\n",
        "    if dfs:\n",
        "        df_final = pd.concat(dfs, ignore_index=True)\n",
        "        caminho_csv = f\"{diretorio_saida}/dengue_{ano_inicio}_{ano_fim}_por_estado.csv\"\n",
        "        df_final.to_csv(caminho_csv, index=False)\n",
        "        print(f\"\\n Dados salvos com sucesso em: {caminho_csv}\")\n",
        "    else:\n",
        "        print(\" Nenhum dado foi coletado.\")"
      ],
      "id": "Uv7bslxSQHh7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**REFINAMENTO DOS DADOS / FEATURES**"
      ],
      "metadata": {
        "id": "7_kW6Fk-T-CS"
      },
      "id": "7_kW6Fk-T-CS"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xibVpBruQHh8"
      },
      "outputs": [],
      "source": [
        "# ===============================================================\n",
        "# === 2. ENGENHARIA DE FEATURES ===\n",
        "# ===============================================================\n",
        "# Objetivo: preparar os dados brutos para uso em modelos de Machine Learning.\n",
        "# As etapas incluem limpeza, criação de variáveis temporais e remoção de outliers.\n",
        "# ===============================================================\n",
        "\n",
        "def carregar_e_limpar(caminho_arquivo):\n",
        "    \"\"\"\n",
        "    Carrega o arquivo CSV contendo os dados de dengue e retorna um DataFrame.\n",
        "\n",
        "    Parâmetros:\n",
        "    - caminho_arquivo (str): Caminho completo para o arquivo CSV.\n",
        "\n",
        "    Retorna:\n",
        "    - df (DataFrame): Dados carregados em formato pandas.\n",
        "    \"\"\"\n",
        "    return pd.read_csv(caminho_arquivo)\n",
        "\n",
        "\n",
        "def adicionar_variaveis_temporais(df):\n",
        "    \"\"\"\n",
        "    Adiciona colunas temporais e variáveis derivadas ao DataFrame.\n",
        "\n",
        "    - Converte a coluna 'data_iniSE' para o formato datetime.\n",
        "    - Cria colunas de ano e semana epidemiológica.\n",
        "    - Cria variáveis defasadas (lags) de 1, 2 e 3 semanas.\n",
        "    - Calcula a média móvel de 3 semanas dos casos.\n",
        "\n",
        "    Parâmetros:\n",
        "    - df (DataFrame): Dados originais com a coluna 'data_iniSE' e 'casos'.\n",
        "\n",
        "    Retorna:\n",
        "    - df (DataFrame): Dados com as novas colunas de engenharia temporal.\n",
        "    \"\"\"\n",
        "    # Conversão da data para formato datetime\n",
        "    df['data_iniSE'] = pd.to_datetime(df['data_iniSE'])\n",
        "\n",
        "    # Criação das variáveis temporais\n",
        "    df['epidemiological_year'] = df['data_iniSE'].dt.year\n",
        "    df['epidemiological_week'] = df['data_iniSE'].dt.isocalendar().week\n",
        "\n",
        "    # Criação de variáveis de defasagem (lags)\n",
        "    df['casos_lag_1'] = df['casos'].shift(1)\n",
        "    df['casos_lag_2'] = df['casos'].shift(2)\n",
        "    df['casos_lag_3'] = df['casos'].shift(3)\n",
        "\n",
        "    # Média móvel dos 3 lags (captura tendência recente)\n",
        "    df['casos_media_3'] = df[['casos_lag_1', 'casos_lag_2', 'casos_lag_3']].mean(axis=1)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def remover_outliers(df, features):\n",
        "    \"\"\"\n",
        "    Remove valores extremos (outliers) das variáveis especificadas.\n",
        "\n",
        "    O método utilizado é baseado no IQR (Interquartile Range),\n",
        "    removendo valores que estejam fora do intervalo [Q1 - 1.5*IQR, Q3 + 1.5*IQR].\n",
        "\n",
        "    Parâmetros:\n",
        "    - df (DataFrame): Conjunto de dados original.\n",
        "    - features (list): Lista de colunas numéricas para aplicar o filtro.\n",
        "\n",
        "    Retorna:\n",
        "    - df (DataFrame): Conjunto de dados sem outliers nas features selecionadas.\n",
        "    \"\"\"\n",
        "    for feature in features:\n",
        "        Q1 = df[feature].quantile(0.25)\n",
        "        Q3 = df[feature].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        df = df[(df[feature] >= Q1 - 1.5 * IQR) & (df[feature] <= Q3 + 1.5 * IQR)]\n",
        "    return df\n"
      ],
      "id": "xibVpBruQHh8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODELOS DE PREDIÇÃO**"
      ],
      "metadata": {
        "id": "nvvZUMJZUB3C"
      },
      "id": "nvvZUMJZUB3C"
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# === 3.1 MODELOS MACHINE LEARNING (CLÁSSICOS) ===\n",
        "# ===============================================================\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def time_weights(ord_vec, power: float = 1.0):\n",
        "    \"\"\"\n",
        "    Gera pesos temporais dando mais peso para semanas mais recentes.\n",
        "\n",
        "    Parâmetros\n",
        "    ----------\n",
        "    ord_vec : array-like\n",
        "        Vetor temporal no formato YYYYWW (ex: 202450).\n",
        "    power : float, default=1.0\n",
        "        Controla o quão agressiva é a ênfase no recente.\n",
        "        - 0 -> todos iguais\n",
        "        - 1 -> leve ênfase\n",
        "        - >1 -> mais peso ainda para recente\n",
        "\n",
        "    Retorna\n",
        "    -------\n",
        "    weights : np.ndarray\n",
        "        Vetor de pesos positivo, normalizado para média = 1.\n",
        "    \"\"\"\n",
        "    ord_vec = np.asarray(ord_vec, dtype=float)\n",
        "    if ord_vec.size == 0:\n",
        "        return np.array([], dtype=float)\n",
        "\n",
        "    o_min = ord_vec.min()\n",
        "    o_max = ord_vec.max()\n",
        "\n",
        "    # evita divisão por zero\n",
        "    if o_max == o_min:\n",
        "        weights = np.ones_like(ord_vec)\n",
        "    else:\n",
        "        # normaliza para [0,1] (0 = mais antigo, 1 = mais recente)\n",
        "        norm = (ord_vec - o_min) / (o_max - o_min)\n",
        "        # transforma em pesos, puxando pra cima (>= 0.5)\n",
        "        weights = 0.5 + 0.5 * (norm ** power)\n",
        "\n",
        "    # normaliza para média 1 (ajuda em treinamento estável)\n",
        "    weights = weights / weights.mean()\n",
        "    return weights\n",
        "\n",
        "def treinar_modelos_classicos(\n",
        "    X: np.ndarray,\n",
        "    y: np.ndarray,\n",
        "    test_size: float = 0.2,\n",
        "    random_state: int = 42\n",
        "):\n",
        "    \"\"\"\n",
        "    Treina quatro modelos de ML clássico (Linear Regression, Random Forest,\n",
        "    Gradient Boosting e XGBoost) e retorna métricas de desempenho.\n",
        "\n",
        "    Parâmetros\n",
        "    ----------\n",
        "    X : np.ndarray\n",
        "        Matriz de features (n_amostras x n_features).\n",
        "    y : np.ndarray\n",
        "        Vetor alvo (n_amostras,).\n",
        "    test_size : float, default=0.2\n",
        "        Proporção dos dados reservada para teste.\n",
        "    random_state : int, default=42\n",
        "        Semente para reprodutibilidade.\n",
        "\n",
        "    Retorna\n",
        "    -------\n",
        "    resultados : dict\n",
        "        Dicionário {nome_modelo: (mse, rmse, r2, nrmse_pct)}.\n",
        "    modelos_treinados : dict\n",
        "        Dicionário {nome_modelo: instancia_treinada}.\n",
        "    \"\"\"\n",
        "    # Split simples (não temporal) — bom como baseline\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=random_state\n",
        "    )\n",
        "\n",
        "    # Conjunto de modelos base com configurações reprodutíveis\n",
        "    modelos = {\n",
        "        \"Linear Regression\": LinearRegression(),\n",
        "        \"Random Forest\": RandomForestRegressor(\n",
        "            n_estimators=300,\n",
        "            random_state=random_state,\n",
        "            n_jobs=-1\n",
        "        ),\n",
        "        \"Gradient Boosting\": GradientBoostingRegressor(\n",
        "            random_state=random_state\n",
        "        ),\n",
        "        \"XGBoost\": XGBRegressor(\n",
        "            random_state=random_state,\n",
        "            n_estimators=600,\n",
        "            learning_rate=0.05,\n",
        "            subsample=0.9,\n",
        "            colsample_bytree=0.9,\n",
        "            reg_lambda=1.0,\n",
        "            tree_method=\"hist\",   # mais rápido; use \"gpu_hist\" se houver GPU\n",
        "        ),\n",
        "    }\n",
        "\n",
        "    resultados = {}\n",
        "    modelos_treinados = {}\n",
        "\n",
        "    for nome, modelo in modelos.items():\n",
        "        # XGBoost com early stopping opcional\n",
        "        if isinstance(modelo, XGBRegressor):\n",
        "            modelo.fit(\n",
        "                X_train, y_train,\n",
        "                eval_set=[(X_test, y_test)],\n",
        "                verbose=False,\n",
        "                early_stopping_rounds=50\n",
        "            )\n",
        "        else:\n",
        "            modelo.fit(X_train, y_train)\n",
        "\n",
        "        # Predição no teste\n",
        "        y_pred = modelo.predict(X_test)\n",
        "\n",
        "        # Métricas\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "        rmse = float(np.sqrt(mse))\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "        media_y = float(np.mean(y_test))\n",
        "        nrmse_pct = (rmse / media_y * 100.0) if media_y != 0 else np.nan\n",
        "\n",
        "        resultados[nome] = (mse, rmse, r2, nrmse_pct)\n",
        "        modelos_treinados[nome] = modelo\n",
        "\n",
        "    return resultados, modelos_treinados\n",
        "\n",
        "    # Treinamento e avaliação padronizados\n",
        "    for nome, modelo in modelos.items():\n",
        "        # Para XGBoost podemos usar early stopping de forma segura\n",
        "        if isinstance(modelo, XGBRegressor):\n",
        "            modelo.fit(\n",
        "                X_train, y_train,\n",
        "                eval_set=[(X_test, y_test)],\n",
        "                verbose=False,\n",
        "                early_stopping_rounds=50\n",
        "            )\n",
        "        else:\n",
        "            modelo.fit(X_train, y_train)\n",
        "\n",
        "        # Predição e métricas\n",
        "        y_pred = modelo.predict(X_test)\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "        rmse = float(np.sqrt(mse))\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "        media_y = float(np.mean(y_test))\n",
        "        nrmse_pct = (rmse / media_y * 100.0) if media_y != 0 else np.nan\n",
        "\n",
        "        resultados[nome] = (mse, rmse, r2, nrmse_pct)\n",
        "        modelos_treinados[nome] = modelo\n",
        "\n",
        "    return resultados, modelos_treinados\n",
        "\n",
        "\n",
        "def resultados_para_dataframe(resultados: dict) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Converte o dicionário de métricas em um DataFrame ordenado por RMSE.\n",
        "    \"\"\"\n",
        "    colunas = [\"Modelo\", \"MSE\", \"RMSE\", \"R2\", \"NRMSE_%\"]\n",
        "    linhas = [\n",
        "        [nome, *metricas] for nome, metricas in resultados.items()\n",
        "    ]\n",
        "    df_res = pd.DataFrame(linhas, columns=colunas)\n",
        "    return df_res.sort_values(\"RMSE\").reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "Q-e1x6q6QlWG"
      },
      "id": "Q-e1x6q6QlWG",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# === 3.2 LSTM — Modelagem Sequencial por Estado (multi-séries) ===\n",
        "# ===============================================================\n",
        "# Ideia central:\n",
        "# 1) Ordenar dados por (estado, ano, semana) e montar janelas (lookback) -> alvo (horizon).\n",
        "# 2) Separar treino/teste por \"cutoff\" temporal (ano, semana).\n",
        "# 3) Escalonar SOMENTE as features (usando apenas treino).\n",
        "# 4) Treinar LSTM e avaliar com MSE, RMSE, R² e nRMSE (%).\n",
        "# ===============================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import date\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks, optimizers\n",
        "\n",
        "\n",
        "def _ord_from_year_week(y, w):\n",
        "    \"\"\"\n",
        "    Gera um número ordenável no formato YYYYWW para facilitar ordenação temporal.\n",
        "    Ex.: ano=2024, semana=5 -> 202405\n",
        "    \"\"\"\n",
        "    return int(f\"{int(y):04d}{int(w):02d}\")\n",
        "\n",
        "\n",
        "def _build_sequences_by_state(df, features, lookback=8, horizon=1):\n",
        "    \"\"\"\n",
        "    Constrói janelas temporais 3D por estado:\n",
        "    - Para cada estado (codigo_ibge), ordena no tempo e cria pares:\n",
        "        X[t-lookback:t, features] -> y[t + horizon - 1]\n",
        "    - Retorna:\n",
        "        X: shape (n_amostras, lookback, n_features)\n",
        "        y: shape (n_amostras,)\n",
        "\n",
        "    Parâmetros:\n",
        "    - df (DataFrame): deve conter colunas 'codigo_ibge', 'epidemiological_year',\n",
        "                      'epidemiological_week' e 'casos', além de 'features'.\n",
        "    - features (list[str]): nomes das colunas de entrada.\n",
        "    - lookback (int): tamanho da janela temporal usada como entrada.\n",
        "    - horizon (int): passo à frente a ser previsto (1 = próxima semana).\n",
        "\n",
        "    Observação:\n",
        "    - Só cria amostras quando existe histórico suficiente (len >= lookback + horizon).\n",
        "    \"\"\"\n",
        "    X_list, y_list = [], []\n",
        "\n",
        "    # Ordenação global por estado/tempo para consistência\n",
        "    df = df.sort_values([\"codigo_ibge\", \"epidemiological_year\", \"epidemiological_week\"]).copy()\n",
        "\n",
        "    for gid, g in df.groupby(\"codigo_ibge\"):\n",
        "        g = g.copy()\n",
        "        # Índice temporal ordenável\n",
        "        g[\"ord\"] = g.apply(lambda r: _ord_from_year_week(r[\"epidemiological_year\"], r[\"epidemiological_week\"]), axis=1)\n",
        "        g = g.sort_values(\"ord\")\n",
        "\n",
        "        # Arrays de entrada (features) e alvo (casos)\n",
        "        feats_arr  = g[features].to_numpy(dtype=float)\n",
        "        target_arr = g[\"casos\"].to_numpy(dtype=float)\n",
        "\n",
        "        # Pula estados com histórico insuficiente\n",
        "        if len(g) < lookback + horizon:\n",
        "            continue\n",
        "\n",
        "        # Deslizamento temporal para gerar janelas\n",
        "        for t in range(lookback, len(g) - horizon + 1):\n",
        "            X_list.append(feats_arr[t - lookback:t])\n",
        "            y_list.append(target_arr[t + horizon - 1])\n",
        "\n",
        "    X = np.array(X_list)  # (n_amostras, lookback, n_features)\n",
        "    y = np.array(y_list)  # (n_amostras,)\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def _temporal_mask_by_cutoff(df, lookback, horizon, cutoff=(2025, 1)):\n",
        "    \"\"\"\n",
        "    Gera uma máscara booleana (True para TREINO, False para TESTE) com base em um\n",
        "    \"corte\" temporal (ano, semana). Qualquer amostra cujo alvo (y) ocorra em\n",
        "    ord_target < ord_cut -> TREINO; caso contrário -> TESTE.\n",
        "\n",
        "    Mantém o MESMO critério de varredura de _build_sequences_by_state para alinhar\n",
        "    1:1 as posições das amostras em X/y com a máscara retornada.\n",
        "    \"\"\"\n",
        "    ord_cut = _ord_from_year_week(*cutoff)\n",
        "    mask_list = []\n",
        "\n",
        "    # Ordena e repete a varredura por estado\n",
        "    for gid, g in df.sort_values([\"codigo_ibge\", \"epidemiological_year\", \"epidemiological_week\"]).groupby(\"codigo_ibge\"):\n",
        "        g = g.copy()\n",
        "        g[\"ord\"] = g.apply(lambda r: _ord_from_year_week(r[\"epidemiological_year\"], r[\"epidemiological_week\"]), axis=1)\n",
        "        g = g.sort_values(\"ord\")\n",
        "\n",
        "        if len(g) < lookback + horizon:\n",
        "            continue\n",
        "\n",
        "        ord_vec = g[\"ord\"].to_numpy()\n",
        "        for t in range(lookback, len(g) - horizon + 1):\n",
        "            ord_target = ord_vec[t + horizon - 1]\n",
        "            mask_list.append(ord_target < ord_cut)  # True -> treino; False -> teste\n",
        "\n",
        "    mask = np.array(mask_list, dtype=bool)\n",
        "    return mask\n",
        "\n",
        "\n",
        "def treinar_lstm_separado(\n",
        "    df_feat_sem_outliers: pd.DataFrame,\n",
        "    features: list,\n",
        "    lookback: int = 8,\n",
        "    horizon: int = 1,\n",
        "    cutoff_test: tuple = (2025, 1),\n",
        "    epochs: int = 80,\n",
        "    batch_size: int = 256,\n",
        "    lr: float = 1e-3,\n",
        "    verbose: int = 1\n",
        "):\n",
        "    \"\"\"\n",
        "    Treina uma LSTM em janelas temporais por estado (multi-séries agregadas).\n",
        "\n",
        "    Parâmetros:\n",
        "    - df_feat_sem_outliers: DataFrame já limpo/filtrado (sem outliers) contendo:\n",
        "        ['codigo_ibge','epidemiological_year','epidemiological_week','casos'] + features\n",
        "    - features: lista de colunas numéricas de entrada.\n",
        "    - lookback: tamanho da janela temporal (semanas) usada como entrada.\n",
        "    - horizon: horizonte de previsão em semanas (1 = próxima semana).\n",
        "    - cutoff_test: (ano, semana) — tudo com alvo >= cutoff vai para TESTE.\n",
        "    - epochs, batch_size, lr, verbose: hiperparâmetros do treinamento.\n",
        "\n",
        "    Retorna:\n",
        "    - resultados_lstm: dict {\"LSTM\": (mse_te, rmse_te, r2_te, nrmse_te)}\n",
        "    - model_lstm: modelo Keras treinado.\n",
        "    - scaler: StandardScaler ajustado sobre as features do TREINO (para reuso em produção).\n",
        "    \"\"\"\n",
        "\n",
        "    # 0) Higiene: garantir numéricos e sem NaN nas features (ffill/bfill conservadora)\n",
        "    df_seq = df_feat_sem_outliers.copy()\n",
        "    df_seq[features] = df_seq[features].astype(float).fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
        "    df_seq[\"casos\"] = df_seq[\"casos\"].astype(float)\n",
        "\n",
        "    # 1) Construir tensores 3D (amostras, lookback, n_features)\n",
        "    X_all, y_all = _build_sequences_by_state(df_seq, features, lookback=lookback, horizon=horizon)\n",
        "    if len(X_all) == 0:\n",
        "        raise ValueError(\n",
        "            \"Não foi possível construir sequências para a LSTM. \"\n",
        "            \"Verifique se há dados suficientes por estado (lookback + horizon).\"\n",
        "        )\n",
        "\n",
        "    # 2) Máscara temporal alinhada (True = treino; False = teste)\n",
        "    mask_train = _temporal_mask_by_cutoff(df_seq, lookback, horizon, cutoff=cutoff_test)\n",
        "    if mask_train.shape[0] != X_all.shape[0]:\n",
        "        raise RuntimeError(\n",
        "            \"Máscara temporal não alinhada com as sequências. \"\n",
        "            \"Garanta que _build_sequences_by_state e _temporal_mask_by_cutoff percorrem os mesmos estados e janelas.\"\n",
        "        )\n",
        "\n",
        "    X_tr, y_tr = X_all[mask_train], y_all[mask_train]\n",
        "    X_te, y_te = X_all[~mask_train], y_all[~mask_train]\n",
        "\n",
        "    # 3) Escalonamento (apenas sobre TREINO) — reshape para 2D, ajusta, e volta pro 3D\n",
        "    n_features = X_tr.shape[-1]\n",
        "    scaler = StandardScaler()\n",
        "    X_tr_2d = X_tr.reshape(-1, n_features)\n",
        "    scaler.fit(X_tr_2d)  # fit apenas com treino\n",
        "\n",
        "    X_tr_scaled = scaler.transform(X_tr_2d).reshape(X_tr.shape)\n",
        "    X_te_scaled = scaler.transform(X_te.reshape(-1, n_features)).reshape(X_te.shape)\n",
        "\n",
        "    # 4) Arquitetura LSTM simples e robusta para séries semanais\n",
        "    def build_lstm(input_shape, lr=lr):\n",
        "        inp = layers.Input(shape=input_shape)\n",
        "        x = layers.LSTM(64, return_sequences=True)(inp)\n",
        "        x = layers.Dropout(0.2)(x)\n",
        "        x = layers.LSTM(32)(x)\n",
        "        x = layers.Dense(16, activation=\"relu\")(x)\n",
        "        out = layers.Dense(1)(x)\n",
        "        model = models.Model(inp, out)\n",
        "        model.compile(optimizer=optimizers.Adam(learning_rate=lr), loss=\"mse\")\n",
        "        return model\n",
        "\n",
        "    model_lstm = build_lstm((lookback, n_features), lr=lr)\n",
        "\n",
        "    # Callbacks: early stopping + ajuste adaptativo da taxa de aprendizado\n",
        "    early = callbacks.EarlyStopping(patience=10, restore_best_weights=True, monitor=\"val_loss\")\n",
        "    plateau = callbacks.ReduceLROnPlateau(patience=5, factor=0.5, min_lr=1e-5, monitor=\"val_loss\")\n",
        "\n",
        "    # 5) Treinamento (validação interna só no treino — teste já foi separado temporalmente)\n",
        "    hist = model_lstm.fit(\n",
        "        X_tr_scaled, y_tr,\n",
        "        validation_split=0.15,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        callbacks=[early, plateau],\n",
        "        verbose=verbose\n",
        "    )\n",
        "\n",
        "    # 6) Avaliação em TREINO e TESTE (métricas padrão)\n",
        "    y_hat_tr = model_lstm.predict(X_tr_scaled, verbose=0).ravel()\n",
        "    y_hat_te = model_lstm.predict(X_te_scaled, verbose=0).ravel()\n",
        "\n",
        "    def _metrics(y_true, y_pred):\n",
        "        mse  = mean_squared_error(y_true, y_pred)\n",
        "        rmse = np.sqrt(mse)\n",
        "        r2   = r2_score(y_true, y_pred)\n",
        "        nrmse_pct = (rmse / np.mean(y_true) * 100) if np.mean(y_true) != 0 else np.nan\n",
        "        return mse, rmse, r2, nrmse_pct\n",
        "\n",
        "    mse_tr, rmse_tr, r2_tr, nrmse_tr = _metrics(y_tr, y_hat_tr)\n",
        "    mse_te, rmse_te, r2_te, nrmse_te = _metrics(y_te, y_hat_te)\n",
        "\n",
        "    print(\"\\n=== LSTM - Métricas (Treino) ===\")\n",
        "    print(f\"MSE (u²): {mse_tr:,.2f} | RMSE: {rmse_tr:,.2f} | R²: {r2_tr*100:,.2f}% | nRMSE: {nrmse_tr:,.2f}%\")\n",
        "    print(\"=== LSTM - Métricas (Teste)  ===\")\n",
        "    print(f\"MSE (u²): {mse_te:,.2f} | RMSE: {rmse_te:,.2f} | R²: {r2_te*100:,.2f}% | nRMSE: {nrmse_te:,.2f}%\")\n",
        "\n",
        "    resultados_lstm = {\"LSTM\": (mse_te, rmse_te, r2_te, nrmse_te)}\n",
        "    return resultados_lstm, model_lstm, scaler\n"
      ],
      "metadata": {
        "id": "VIUSiFrVLZhR"
      },
      "id": "VIUSiFrVLZhR",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PLOTAGEM E AVALIAÇÃO DOS MODELOS**"
      ],
      "metadata": {
        "id": "4wm1EB9LTQbF"
      },
      "id": "4wm1EB9LTQbF"
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# === 4. VISUALIZAÇÃO DE COMPARAÇÃO DE ERROS ===\n",
        "# ===============================================================\n",
        "# Objetivo: comparar o desempenho (erro percentual) entre os modelos de ML\n",
        "# e Deep Learning (LSTM) através de um gráfico de barras.\n",
        "# ===============================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "def plot_comparacao_erros(modelos, mse_percentuais):\n",
        "    \"\"\"\n",
        "    Gera um gráfico de barras comparando o erro percentual (ex: nRMSE%) entre modelos.\n",
        "\n",
        "    Parâmetros:\n",
        "    - modelos (list[str]): nomes dos modelos (ex: [\"Linear\", \"RF\", \"GB\", \"XGB\", \"LSTM\"])\n",
        "    - mse_percentuais (list[float]): erros normalizados em porcentagem (%)\n",
        "\n",
        "    Retorna:\n",
        "    - None (gera o gráfico com seaborn/matplotlib)\n",
        "    \"\"\"\n",
        "    # Criação do DataFrame para o gráfico\n",
        "    df = pd.DataFrame({\"Modelo\": modelos, \"Erro (%)\": mse_percentuais})\n",
        "\n",
        "    # Tamanho e estilo da figura\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.set_style(\"whitegrid\")\n",
        "\n",
        "    # Gráfico de barras\n",
        "    sns.barplot(\n",
        "        data=df,\n",
        "        x=\"Modelo\",\n",
        "        y=\"Erro (%)\",\n",
        "        hue=\"Modelo\",\n",
        "        palette=\"viridis\",\n",
        "        legend=False\n",
        "    )\n",
        "\n",
        "    # Título e rótulos\n",
        "    plt.title(\"Erro Quadrático Médio Normalizado (%) por Modelo\", fontsize=14, weight=\"bold\")\n",
        "    plt.ylabel(\"Erro (%)\", fontsize=12)\n",
        "    plt.xlabel(\"Modelos\", fontsize=12)\n",
        "\n",
        "    # Ajustes visuais\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "M3xA3R8UQlxl"
      },
      "id": "M3xA3R8UQlxl",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MAIN**"
      ],
      "metadata": {
        "id": "C9fn-A-7U0fK"
      },
      "id": "C9fn-A-7U0fK"
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# === 5. EXECUÇÃO COMPLETA  (TS-SPLIT, XGB Poisson, LSTM e PLOT MENSAL) ===\n",
        "# ===============================================================\n",
        "# Este pipeline integra:\n",
        "# 1) Download e preparação dos dados\n",
        "# 2) Engenharia de features (lags, médias móveis, componentes sazonais)\n",
        "# 3) Treinamento temporal de modelos clássicos (+ XGB Poisson com early stopping)\n",
        "# 4) Treinamento LSTM em janelas por estado e previsão iterativa\n",
        "# 5) Geração de previsões futuras (semanas -> agregação mensal) e plot\n",
        "# Observação: requer as funções definidas previamente:\n",
        "#   - baixar_dados_por_estado, carregar_e_limpar, adicionar_variaveis_temporais, remover_outliers\n",
        "#   - treinar_lstm_separado (ou a versão abaixo já incluída no arquivo)\n",
        "# ===============================================================\n",
        "\n",
        "# --- Imports centrais do pipeline (redundantes caso já estejam no topo do seu script) ---\n",
        "import os\n",
        "from glob import glob\n",
        "from datetime import date, timedelta, datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patheffects as pe\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "# xgboost\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# modelos clássicos e métricas\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# LSTM\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks, optimizers\n",
        "from tensorflow.keras.losses import Huber  # Perda robusta para lidar com outliers\n",
        "\n",
        "# ---- (Opcional) Reprodutibilidade leve ----\n",
        "import random\n",
        "np.random.seed(42); random.seed(42); tf.random.set_seed(42)\n",
        "\n",
        "# --------------------------\n",
        "# Configuração de anos (um único range para todo o pipeline)\n",
        "# --------------------------\n",
        "ANO_INICIO = 2010\n",
        "ANO_FIM    = 2025  # use year atual se preferir: datetime.now().year\n",
        "\n",
        "# ====== Horizonte de previsão (semanas) ======\n",
        "# 26 ≈ 6 meses | 52 ≈ 1 ano\n",
        "N_FUTURO_SEMANAS = 52\n",
        "\n",
        "# --------------------------\n",
        "# Helpers gerais\n",
        "# --------------------------\n",
        "def yw_to_month_start(y, w):\n",
        "    \"\"\"Converte (ano ISO, semana ISO) no primeiro dia do mês correspondente.\n",
        "       Resiliente à semana 53.\"\"\"\n",
        "    y, w = int(y), int(w)\n",
        "    try:\n",
        "        d = date.fromisocalendar(y, w, 1)\n",
        "    except ValueError:\n",
        "        d = date.fromisocalendar(y, 52, 1)  # fallback em anos sem iso-week 53\n",
        "    return d.replace(day=1)\n",
        "\n",
        "def _ord_from_year_week(y, w):\n",
        "    \"\"\"YYYYWW inteiro para ordenações/splits temporais.\"\"\"\n",
        "    return int(f\"{int(y):04d}{int(w):02d}\")\n",
        "\n",
        "# ===== Engenharia de features rápida =====\n",
        "def add_extra_features(df):\n",
        "    g = df.sort_values(\n",
        "        [\"codigo_ibge\",\"epidemiological_year\",\"epidemiological_week\"]\n",
        "    ).copy()\n",
        "\n",
        "    # --- lags de casos\n",
        "    for lag in [1,2,3,4,8,12]:\n",
        "        g[f\"casos_lag_{lag}\"] = g.groupby(\"codigo_ibge\")[\"casos\"].shift(lag)\n",
        "\n",
        "    # --- médias móveis de casos (curtas)\n",
        "    g[\"casos_mm3\"] = g.groupby(\"codigo_ibge\")[\"casos\"].rolling(3).mean().reset_index(0,drop=True)\n",
        "    g[\"casos_mm5\"] = g.groupby(\"codigo_ibge\")[\"casos\"].rolling(5).mean().reset_index(0,drop=True)\n",
        "\n",
        "    # --- sazonalidade (Fourier ordem 1)\n",
        "    w = g[\"epidemiological_week\"].astype(float)\n",
        "    g[\"week_sin\"] = np.sin(2*np.pi*w/52.18)\n",
        "    g[\"week_cos\"] = np.cos(2*np.pi*w/52.18)\n",
        "\n",
        "    # --- clima com lags longos (picos de dengue costumam vir 4–8 semanas depois)\n",
        "    for col in [\"tempmin\",\"umidmax\"]:\n",
        "        if col in g.columns:\n",
        "            for lag in [1,2,4,6,8]:\n",
        "                g[f\"{col}_lag{lag}\"] = g.groupby(\"codigo_ibge\")[col].shift(lag)\n",
        "            # médias móveis climáticas (inércia de ambiente)\n",
        "            g[f\"{col}_mm4\"] = g.groupby(\"codigo_ibge\")[col].rolling(4).mean().reset_index(0,drop=True)\n",
        "            g[f\"{col}_mm8\"] = g.groupby(\"codigo_ibge\")[col].rolling(8).mean().reset_index(0,drop=True)\n",
        "\n",
        "    return g\n",
        "\n",
        "# --------------------------\n",
        "# LSTM — construção de janelas e máscara temporal (versão local do bloco 3.2)\n",
        "# --------------------------\n",
        "def _build_sequences_by_state(df, features, lookback=8, horizon=1):\n",
        "    \"\"\"Monta tensores 3D (amostras, lookback, n_features) por estado.\"\"\"\n",
        "    X_list, y_list = [], []\n",
        "    df = df.sort_values([\"codigo_ibge\",\"epidemiological_year\",\"epidemiological_week\"]).copy()\n",
        "    for gid, g in df.groupby(\"codigo_ibge\"):\n",
        "        g = g.copy()\n",
        "        g[\"ord\"] = g.apply(lambda r: _ord_from_year_week(r[\"epidemiological_year\"], r[\"epidemiological_week\"]), axis=1)\n",
        "        g = g.sort_values(\"ord\")\n",
        "        feats_arr = g[features].to_numpy(dtype=float)\n",
        "        target_arr = g[\"casos\"].to_numpy(dtype=float)\n",
        "        if len(g) < lookback + horizon:\n",
        "            continue\n",
        "        for t in range(lookback, len(g) - horizon + 1):\n",
        "            X_list.append(feats_arr[t - lookback:t])\n",
        "            y_list.append(target_arr[t + horizon - 1])\n",
        "    X = np.array(X_list); y = np.array(y_list)\n",
        "    return X, y\n",
        "\n",
        "def _temporal_mask_by_cutoff(df, lookback, horizon, cutoff=(2025,1)):\n",
        "    \"\"\"Máscara booleana alinhada às janelas: True=train; False=test, por cutoff ISO.\"\"\"\n",
        "    ord_cut = _ord_from_year_week(*cutoff)\n",
        "    mask_list = []\n",
        "    for gid, g in df.sort_values([\"codigo_ibge\",\"epidemiological_year\",\"epidemiological_week\"]).groupby(\"codigo_ibge\"):\n",
        "        g = g.copy()\n",
        "        g[\"ord\"] = g.apply(lambda r: _ord_from_year_week(r[\"epidemiological_year\"], r[\"epidemiological_week\"]), axis=1)\n",
        "        g = g.sort_values(\"ord\")\n",
        "        if len(g) < lookback + horizon:\n",
        "            continue\n",
        "        ord_vec = g[\"ord\"].to_numpy()\n",
        "        for t in range(lookback, len(g) - horizon + 1):\n",
        "            ord_target = ord_vec[t + horizon - 1]\n",
        "            mask_list.append(ord_target < ord_cut)\n",
        "    return np.array(mask_list, dtype=bool)\n",
        "\n",
        "def treinar_lstm_separado(\n",
        "    df_feat_sem_outliers,\n",
        "    features,\n",
        "    lookback=12,\n",
        "    horizon=1,\n",
        "    cutoff_test=(2025,1),\n",
        "    epochs=120,\n",
        "    batch_size=256,\n",
        "    lr=1e-3,\n",
        "    verbose=1\n",
        "):\n",
        "    \"\"\"\n",
        "    Treina LSTM agregando janelas de todas as capitais (multi-séries).\n",
        "    Retorna métricas de teste, o modelo e o scaler para reuso em previsão.\n",
        "    \"\"\"\n",
        "    df_seq = df_feat_sem_outliers.copy()\n",
        "    df_seq[features] = df_seq[features].ffill().bfill()\n",
        "    df_seq[\"casos\"] = df_seq[\"casos\"].astype(float)\n",
        "\n",
        "    X_all, y_all = _build_sequences_by_state(df_seq, features, lookback=lookback, horizon=horizon)\n",
        "    if len(X_all) == 0:\n",
        "        raise ValueError(\"Sem dados suficientes para LSTM. Verifique lookback/horizon.\")\n",
        "\n",
        "    mask_train = _temporal_mask_by_cutoff(df_seq, lookback, horizon, cutoff=cutoff_test)\n",
        "    if mask_train.shape[0] != X_all.shape[0]:\n",
        "        raise RuntimeError(\"Máscara temporal desalinhada.\")\n",
        "\n",
        "    X_tr, y_tr = X_all[mask_train], y_all[mask_train]\n",
        "    X_te, y_te = X_all[~mask_train], y_all[~mask_train]\n",
        "\n",
        "    n_features = X_tr.shape[-1]\n",
        "    scaler = StandardScaler()\n",
        "    X_tr_2d = X_tr.reshape(-1, n_features)\n",
        "    scaler.fit(X_tr_2d)\n",
        "    X_tr_scaled = scaler.transform(X_tr_2d).reshape(X_tr.shape)\n",
        "    X_te_scaled = scaler.transform(X_te.reshape(-1, n_features)).reshape(X_te.shape)\n",
        "\n",
        "    def build_lstm(input_shape, lr=lr):\n",
        "        inp = layers.Input(shape=input_shape)\n",
        "        x = layers.LSTM(64, return_sequences=True)(inp)\n",
        "        x = layers.Dropout(0.2)(x)\n",
        "        x = layers.LSTM(32)(x)\n",
        "        x = layers.Dense(16, activation=\"relu\")(x)\n",
        "        out = layers.Dense(1, activation=\"softplus\")(x)  # saída positiva e suave\n",
        "        model = models.Model(inp, out)\n",
        "        model.compile(optimizer=optimizers.Adam(learning_rate=lr), loss=Huber(delta=1.0))\n",
        "        return model\n",
        "\n",
        "    model_lstm = build_lstm((lookback, n_features), lr=lr)\n",
        "    early = callbacks.EarlyStopping(patience=10, restore_best_weights=True, monitor=\"val_loss\")\n",
        "    plateau = callbacks.ReduceLROnPlateau(patience=5, factor=0.5, min_lr=1e-5, monitor=\"val_loss\")\n",
        "\n",
        "    model_lstm.fit(\n",
        "        X_tr_scaled, y_tr,\n",
        "        validation_split=0.15,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        callbacks=[early, plateau],\n",
        "        verbose=verbose\n",
        "    )\n",
        "\n",
        "    def _metrics(y_true, y_pred):\n",
        "        mse = np.mean((y_true - y_pred) ** 2)\n",
        "        rmse = float(np.sqrt(mse))\n",
        "        ss_res = np.sum((y_true - y_pred) ** 2)\n",
        "        ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
        "        r2 = 1 - ss_res / ss_tot if ss_tot != 0 else np.nan\n",
        "        nrmse_pct = (rmse / np.mean(y_true)) * 100 if np.mean(y_true) != 0 else np.nan\n",
        "        return mse, rmse, r2, nrmse_pct\n",
        "\n",
        "    y_hat_tr = model_lstm.predict(X_tr_scaled, verbose=0).ravel()\n",
        "    y_hat_te = model_lstm.predict(X_te_scaled, verbose=0).ravel()\n",
        "\n",
        "    mse_tr, rmse_tr, r2_tr, nrmse_tr = _metrics(y_tr, y_hat_tr)\n",
        "    mse_te, rmse_te, r2_te, nrmse_te = _metrics(y_te, y_hat_te)\n",
        "\n",
        "    print(\"\\n=== LSTM - Métricas (Treino) ===\")\n",
        "    print(f\"MSE (u²): {mse_tr:,.2f} | RMSE: {rmse_tr:,.2f} | R²: {r2_tr*100:,.2f}% | nRMSE: {nrmse_tr:,.2f}%\")\n",
        "    print(\"=== LSTM - Métricas (Teste)  ===\")\n",
        "    print(f\"MSE (u²): {mse_te:,.2f} | RMSE: {rmse_te:,.2f} | R²: {r2_te*100:,.2f}% | nRMSE: {nrmse_te:,.2f}%\")\n",
        "\n",
        "    resultados_lstm = {\"LSTM\": (mse_te, rmse_te, r2_te, nrmse_te)}\n",
        "    return resultados_lstm, model_lstm, scaler, lookback\n",
        "\n",
        "# ===== previsão LSTM para N semanas (genérica) =====\n",
        "def prever_n_semanas_lstm(df_feat_sem_outliers, features, model_lstm, scaler, lookback, n_weeks=26):\n",
        "    \"\"\"\n",
        "    Faz previsão iterativa semana-a-semana, por estado, atualizando lags compatíveis.\n",
        "    Retorna DataFrame com colunas: codigo_ibge, epidemiological_year, epidemiological_week, casos_previstos_lstm\n",
        "    \"\"\"\n",
        "    df = df_feat_sem_outliers.sort_values([\"codigo_ibge\",\"epidemiological_year\",\"epidemiological_week\"]).copy()\n",
        "    last_windows = {}\n",
        "    for gid, g in df.groupby(\"codigo_ibge\"):\n",
        "        if len(g) < lookback:\n",
        "            continue\n",
        "        last_windows[gid] = g.iloc[-lookback:].copy()\n",
        "\n",
        "    fut_rows = []\n",
        "    # índices em 'features' que precisam ser atualizados durante a simulação\n",
        "    idx_lag1 = features.index(\"casos_lag_1\") if \"casos_lag_1\" in features else None\n",
        "    idx_med3 = features.index(\"casos_media_3\") if \"casos_media_3\" in features else None\n",
        "\n",
        "    for gid, win_df in last_windows.items():\n",
        "        win_vals = win_df[features].to_numpy(dtype=float)  # (lookback, n_features)\n",
        "        y_last = int(win_df.iloc[-1][\"epidemiological_year\"])\n",
        "        w_last = int(win_df.iloc[-1][\"epidemiological_week\"])\n",
        "\n",
        "        for _ in range(n_weeks):\n",
        "            X_in = scaler.transform(win_vals)\n",
        "            y_pred = float(model_lstm.predict(X_in[np.newaxis, ...], verbose=0).ravel()[0])\n",
        "            y_pred = max(0.0, y_pred)\n",
        "            # aplica teto FUT_CAP se definido globalmente\n",
        "            if 'FUT_CAP' in globals() and FUT_CAP is not None:\n",
        "                y_pred = min(y_pred, FUT_CAP)\n",
        "\n",
        "            # atualiza lags/médias para a próxima janela\n",
        "            new_row = win_vals[-1].copy()\n",
        "            if idx_lag1 is not None:\n",
        "                new_row[idx_lag1] = y_pred\n",
        "            if idx_med3 is not None:\n",
        "                last_lag1_series = win_vals[-2:, idx_lag1] if win_vals.shape[0] >= 2 else np.array([])\n",
        "                vals = list(last_lag1_series) + [y_pred]\n",
        "                new_row[idx_med3] = np.mean(vals)\n",
        "\n",
        "            # avança 1 semana ISO\n",
        "            d = date.fromisocalendar(y_last, w_last, 1) + timedelta(weeks=1)\n",
        "            iso = d.isocalendar(); y_last, w_last = int(iso[0]), int(iso[1])\n",
        "\n",
        "            fut_rows.append({\n",
        "                \"codigo_ibge\": gid,\n",
        "                \"epidemiological_year\": y_last,\n",
        "                \"epidemiological_week\": w_last,\n",
        "                \"casos_previstos_lstm\": y_pred\n",
        "            })\n",
        "            # janela deslizante\n",
        "            win_vals = np.vstack([win_vals[1:], new_row])\n",
        "\n",
        "    return pd.DataFrame(fut_rows)\n",
        "\n",
        "# --------------------------\n",
        "# Modelos clássicos com split temporal + XGB Poisson (early stopping)\n",
        "# --------------------------\n",
        "def treinar_modelos(X, y, ord_vec, val_weeks=26):\n",
        "    \"\"\"\n",
        "    Split temporal: últimas `val_weeks` semanas = validação.\n",
        "    Retorna:\n",
        "        resultados: dict {nome_modelo: (mse, r2)}\n",
        "        modelos_treinados: dict {nome_modelo: estimator_com_predict}\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "    from sklearn.linear_model import LinearRegression\n",
        "    from sklearn.metrics import mean_squared_error, r2_score\n",
        "    import xgboost as xgb\n",
        "    from xgboost import XGBRegressor\n",
        "\n",
        "    # --- split temporal\n",
        "    ords = np.sort(np.unique(ord_vec))\n",
        "    if len(ords) == 0:\n",
        "        raise ValueError(\"ord_vec vazio.\")\n",
        "\n",
        "    val_ords = ords[-val_weeks:] if len(ords) > val_weeks else ords[-max(1, len(ords)//5):]\n",
        "    mask_val = np.isin(ord_vec, val_ords)\n",
        "    mask_tr  = ~mask_val\n",
        "\n",
        "    X_tr, y_tr = X[mask_tr], y[mask_tr]\n",
        "    X_val, y_val = X[mask_val], y[mask_val]\n",
        "\n",
        "    # --- pesos temporais (levemente mais peso p/ recente)\n",
        "    w_tr = time_weights(ord_vec[mask_tr], power=1.0)\n",
        "\n",
        "    # --- modelos sklearn \"clássicos\"\n",
        "    modelos = {\n",
        "      #  \"Linear Regression\": LinearRegression(),\n",
        "        \"Random Forest\": RandomForestRegressor(\n",
        "            random_state=42, n_estimators=800, max_features=\"sqrt\",\n",
        "            max_depth=12, min_samples_leaf=5\n",
        "        ),\n",
        "        \"Gradient Boosting\": GradientBoostingRegressor(\n",
        "            random_state=42, n_estimators=1000, learning_rate=0.05, max_depth=3\n",
        "        ),\n",
        "        \"XGBoost\": XGBRegressor(\n",
        "            random_state=42, n_estimators=1500, learning_rate=0.03, max_depth=4,\n",
        "            min_child_weight=5, subsample=0.9, colsample_bytree=0.9,\n",
        "            reg_lambda=2.0, reg_alpha=0.1, tree_method=\"hist\"\n",
        "        ),\n",
        "    }\n",
        "\n",
        "    resultados = {}\n",
        "    modelos_treinados = {}\n",
        "\n",
        "    # --- treina modelos clássicos\n",
        "    for nome, modelo in modelos.items():\n",
        "        try:\n",
        "            # Linear Regression: NÃO usa sample_weight\n",
        "            if isinstance(modelo, LinearRegression):\n",
        "                modelo.fit(X_tr, y_tr)\n",
        "            else:\n",
        "                # demais modelos: tenta usar sample_weight; se não aceitar, vai sem\n",
        "                try:\n",
        "                    modelo.fit(X_tr, y_tr, sample_weight=w_tr)\n",
        "                except TypeError:\n",
        "                    modelo.fit(X_tr, y_tr)\n",
        "\n",
        "            # predição na validação temporal\n",
        "            y_hat = modelo.predict(X_val)\n",
        "            mse = mean_squared_error(y_val, y_hat)\n",
        "            r2  = r2_score(y_val, y_hat)\n",
        "\n",
        "            resultados[nome] = (float(mse), float(r2))\n",
        "            modelos_treinados[nome] = modelo\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[AVISO] Falha ao treinar {nome}: {e}\")\n",
        "\n",
        "    # --- XGBoost Poisson com early stopping via xgb.train\n",
        "    try:\n",
        "        dtr  = xgb.DMatrix(X_tr, label=y_tr, weight=w_tr)\n",
        "        dval = xgb.DMatrix(X_val, label=y_val)\n",
        "\n",
        "        params = {\n",
        "            \"objective\": \"count:poisson\",\n",
        "            \"eval_metric\": \"poisson-nloglik\",\n",
        "            \"tree_method\": \"hist\",\n",
        "            \"random_state\": 42,\n",
        "            \"eta\": 0.03,\n",
        "            \"max_depth\": 4,\n",
        "            \"min_child_weight\": 5,\n",
        "            \"subsample\": 0.9,\n",
        "            \"colsample_bytree\": 0.9,\n",
        "            \"lambda\": 2.0,   # L2\n",
        "            \"alpha\": 0.1     # L1\n",
        "        }\n",
        "\n",
        "        booster = xgb.train(\n",
        "            params,\n",
        "            dtr,\n",
        "            num_boost_round=5000,\n",
        "            evals=[(dval, \"val\")],\n",
        "            early_stopping_rounds=200,\n",
        "            verbose_eval=False\n",
        "        )\n",
        "\n",
        "        class XGBPoissonWrapper:\n",
        "            def __init__(self, booster):\n",
        "                self.booster = booster\n",
        "            def predict(self, X_):\n",
        "                return self.booster.predict(xgb.DMatrix(X_))\n",
        "\n",
        "        xgb_poisson_model = XGBPoissonWrapper(booster)\n",
        "        y_hat_p = xgb_poisson_model.predict(X_val)\n",
        "        mse_p = mean_squared_error(y_val, y_hat_p)\n",
        "        r2_p  = r2_score(y_val, y_hat_p)\n",
        "\n",
        "        resultados[\"XGBoost Poisson (TS-ES)\"] = (float(mse_p), float(r2_p))\n",
        "        modelos_treinados[\"XGBoost Poisson (TS-ES)\"] = xgb_poisson_model\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[AVISO] Falha no XGBoost Poisson (TS-ES): {e}\")\n",
        "\n",
        "    # --- sanity check e retorno GARANTIDO\n",
        "    if not resultados:\n",
        "        raise RuntimeError(\"Nenhum modelo foi treinado com sucesso; 'resultados' ficou vazio.\")\n",
        "\n",
        "    return resultados, modelos_treinados\n",
        "\n",
        "    # --- treina modelos clássicos com proteção de erro\n",
        "    for nome, modelo in modelos.items():\n",
        "        try:\n",
        "            try:\n",
        "                modelo.fit(X_tr, y_tr, sample_weight=w_tr)\n",
        "            except TypeError:\n",
        "                modelo.fit(X_tr, y_tr)\n",
        "            y_hat = modelo.predict(X_val)\n",
        "            mse = mean_squared_error(y_val, y_hat)\n",
        "            r2  = r2_score(y_val, y_hat)\n",
        "            resultados[nome] = (float(mse), float(r2))\n",
        "            modelos_treinados[nome] = modelo\n",
        "        except Exception as e:\n",
        "            print(f\"[AVISO] Falha ao treinar {nome}: {e}\")\n",
        "\n",
        "    # --- XGBoost Poisson com early stopping via xgb.train\n",
        "    try:\n",
        "        dtr  = xgb.DMatrix(X_tr, label=y_tr, weight=w_tr)\n",
        "        dval = xgb.DMatrix(X_val, label=y_val)\n",
        "\n",
        "        params = {\n",
        "            \"objective\": \"count:poisson\",\n",
        "            \"eval_metric\": \"poisson-nloglik\",\n",
        "            \"tree_method\": \"hist\",\n",
        "            \"random_state\": 42,\n",
        "            \"eta\": 0.03,\n",
        "            \"max_depth\": 4,\n",
        "            \"min_child_weight\": 5,\n",
        "            \"subsample\": 0.9,\n",
        "            \"colsample_bytree\": 0.9,\n",
        "            \"lambda\": 2.0,   # L2\n",
        "            \"alpha\": 0.1     # L1\n",
        "        }\n",
        "\n",
        "        booster = xgb.train(\n",
        "            params,\n",
        "            dtr,\n",
        "            num_boost_round=5000,\n",
        "            evals=[(dval, \"val\")],\n",
        "            early_stopping_rounds=200,\n",
        "            verbose_eval=False\n",
        "        )\n",
        "\n",
        "        class XGBPoissonWrapper:\n",
        "            def __init__(self, booster):\n",
        "                self.booster = booster\n",
        "            def predict(self, X_):\n",
        "                return self.booster.predict(xgb.DMatrix(X_))\n",
        "\n",
        "        xgb_poisson_model = XGBPoissonWrapper(booster)\n",
        "        y_hat_p = xgb_poisson_model.predict(X_val)\n",
        "        mse_p = mean_squared_error(y_val, y_hat_p)\n",
        "        r2_p  = r2_score(y_val, y_hat_p)\n",
        "\n",
        "        resultados[\"XGBoost Poisson (TS-ES)\"] = (float(mse_p), float(r2_p))\n",
        "        modelos_treinados[\"XGBoost Poisson (TS-ES)\"] = xgb_poisson_model\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[AVISO] Falha no XGBoost Poisson (TS-ES): {e}\")\n",
        "\n",
        "    # --- sanity check e retorno GARANTIDO\n",
        "    if not resultados:\n",
        "        raise RuntimeError(\"Nenhum modelo foi treinado com sucesso; 'resultados' ficou vazio.\")\n",
        "\n",
        "    return resultados, modelos_treinados\n",
        "\n",
        "# --------------------------\n",
        "# Fluxo principal\n",
        "# --------------------------\n",
        "\n",
        "# Passo 1 — Download com o mesmo range usado no restante do pipeline\n",
        "baixar_dados_por_estado(ano_inicio=ANO_INICIO, ano_fim=ANO_FIM)\n",
        "\n",
        "# Passo 2 — Carregar o arquivo correto (com fallback se não existir)\n",
        "caminho_csv = f\"data/raw/dengue_{ANO_INICIO}_{ANO_FIM}_por_estado.csv\"\n",
        "if not os.path.exists(caminho_csv):\n",
        "    candidatos = sorted(glob(\"data/raw/dengue_*_por_estado.csv\"))\n",
        "    if candidatos:\n",
        "        print(f\" Arquivo {caminho_csv} não encontrado. Usando {candidatos[-1]}\")\n",
        "        caminho_csv = candidatos[-1]\n",
        "    else:\n",
        "        raise FileNotFoundError(\"Nenhum CSV de dengue encontrado em data/raw/. Verifique o download.\")\n",
        "\n",
        "# Carrega e aplica engenharia básica\n",
        "df = carregar_e_limpar(caminho_csv)\n",
        "df_feat = adicionar_variaveis_temporais(df)\n",
        "df_feat = add_extra_features(df_feat)  # novas features\n",
        "\n",
        "# Preenche NaNs (lags iniciais e possíveis faltas de clima/população)\n",
        "df_feat = df_feat.sort_values([\"codigo_ibge\",\"epidemiological_year\",\"epidemiological_week\"]).copy()\n",
        "cols_to_fill = list(set([\n",
        "    \"casos\",\n",
        "    \"casos_lag_1\",\"casos_lag_2\",\"casos_lag_3\",\"casos_lag_4\",\"casos_lag_8\",\"casos_lag_12\",\n",
        "    \"casos_mm3\",\n",
        "    \"week_sin\",\"week_cos\",\n",
        "    \"pop\",\"tempmin_lag1\",\"tempmin_lag2\",\"umidmax_lag1\",\"umidmax_lag2\",\n",
        "    \"receptivo\",\"transmissao\",\n",
        "    \"casos_media_3\"\n",
        "]))\n",
        "\n",
        "for col in cols_to_fill:\n",
        "    if col in df_feat.columns:\n",
        "        df_feat[col] = df_feat.groupby(\"codigo_ibge\")[col].transform(lambda s: s.ffill().bfill())\n",
        "\n",
        "# Se \"casos_media_3\" não existia, cria a partir dos lags\n",
        "if \"casos_media_3\" in df_feat.columns:\n",
        "    mask_missing = df_feat[\"casos_media_3\"].isna()\n",
        "    if mask_missing.any():\n",
        "        df_feat.loc[mask_missing, \"casos_media_3\"] = (\n",
        "            df_feat.loc[mask_missing, [\"casos_lag_1\",\"casos_lag_2\",\"casos_lag_3\"]].mean(axis=1)\n",
        "        )\n",
        "\n",
        "# Seleção de features (inclui as novas criadas)\n",
        "# Lista de features desejadas\n",
        "features_desejadas = [\n",
        "    # casos\n",
        "    \"casos_lag_1\",\"casos_lag_2\",\"casos_lag_3\",\"casos_lag_4\",\"casos_lag_8\",\"casos_lag_12\",\n",
        "    \"casos_mm3\",\"casos_mm5\",\n",
        "    # sazonalidade\n",
        "    \"week_sin\",\"week_cos\",\n",
        "    # demografia/clima (atuais)\n",
        "    \"pop\",\"tempmin\",\"umidmax\",\n",
        "    # clima defasado e suavizado\n",
        "    \"tempmin_lag1\",\"tempmin_lag2\",\"tempmin_lag4\",\"tempmin_lag6\",\"tempmin_lag8\",\"tempmin_mm4\",\"tempmin_mm8\",\n",
        "    \"umidmax_lag1\",\"umidmax_lag2\",\"umidmax_lag4\",\"umidmax_lag6\",\"umidmax_lag8\",\"umidmax_mm4\",\"umidmax_mm8\",\n",
        "    # receptividade/transmissão (se existirem)\n",
        "    \"receptivo\",\"transmissao\",\n",
        "    # compat LSTM\n",
        "    \"casos_media_3\"\n",
        "]\n",
        "\n",
        "# Mantém apenas as colunas que realmente existem no df_feat\n",
        "features = [col for col in features_desejadas if col in df_feat.columns]\n",
        "\n",
        "print(\"Features usadas no pipeline:\")\n",
        "print(features)\n",
        "\n",
        "\n",
        "# Remoção de outliers nas features (conforme seu bloco 2)\n",
        "df_feat_sem_outliers = remover_outliers(df_feat, features)\n",
        "\n",
        "# Vetor temporal para split/weights\n",
        "ord_vec = (df_feat_sem_outliers[\"epidemiological_year\"].astype(int)*100 +\n",
        "           df_feat_sem_outliers[\"epidemiological_week\"].astype(int)).values\n",
        "\n",
        "# Matrizes de treino\n",
        "X = df_feat_sem_outliers[features].values\n",
        "y = df_feat_sem_outliers[\"casos\"].values.astype(float)\n",
        "\n",
        "# Teto de segurança para previsões futuras (evita explosões)\n",
        "p98 = np.percentile(y[~np.isnan(y)], 98)\n",
        "FUT_CAP = float(p98 * 1.75)  # era 1.25\n",
        "\n",
        "# Treino temporal + Early Stopping\n",
        "resultados, modelos_treinados = treinar_modelos(X, y, ord_vec, val_weeks=26)\n",
        "\n",
        "# Escolha automática do melhor (menor MSE)\n",
        "modelo_escolhido_nome = min(resultados.keys(), key=lambda k: resultados[k][0])\n",
        "modelo_escolhido = modelos_treinados[modelo_escolhido_nome]\n",
        "print(f\"\\n>>> Modelo escolhido: {modelo_escolhido_nome}\")\n",
        "\n",
        "# ===== Avaliação (val temporal) — gráfico de MSE\n",
        "df_mse = (\n",
        "    pd.DataFrame([(k, v[0]) for k, v in resultados.items()], columns=[\"Modelo\",\"MSE\"])\n",
        "      .sort_values(\"MSE\")\n",
        ")\n",
        "sns.barplot(data=df_mse, x=\"Modelo\", y=\"MSE\", hue=\"Modelo\", palette=\"viridis\", legend=False)\n",
        "plt.title(\"Comparação de MSE na Validação Temporal\")\n",
        "plt.ylabel(\"MSE\"); plt.xlabel(\"Modelo\"); plt.xticks(rotation=45, ha='right'); plt.tight_layout(); plt.show()\n",
        "\n",
        "# ===== Previsões no histórico (in-sample) com o modelo escolhido\n",
        "df_feat_sem_outliers[\"casos_previstos_in_sample\"] = np.clip(modelo_escolhido.predict(X), 0, None)\n",
        "\n",
        "# ===== Função para avançar 1 semana ISO (vetorizado)\n",
        "def advance_iso_week_vectorized(years, weeks):\n",
        "    new_years, new_weeks = [], []\n",
        "    for y_, w_ in zip(years, weeks):\n",
        "        d = date.fromisocalendar(int(y_), int(w_), 1) + timedelta(weeks=1)\n",
        "        iso = d.isocalendar()\n",
        "        new_years.append(int(iso[0])); new_weeks.append(int(iso[1]))\n",
        "    return pd.Series(new_years), pd.Series(new_weeks)\n",
        "\n",
        "# ===== Preparação dos últimos registros por estado (seed de previsão)\n",
        "ultimos_registros = (\n",
        "    df_feat_sem_outliers\n",
        "    .sort_values([\"codigo_ibge\",\"epidemiological_year\",\"epidemiological_week\"])\n",
        "    .groupby(\"codigo_ibge\").tail(1).copy()\n",
        ")\n",
        "# Garante colunas de lags fundamentais\n",
        "for col in [\"casos_lag_1\",\"casos_lag_2\",\"casos_lag_3\",\"casos_media_3\"]:\n",
        "    if col not in ultimos_registros.columns:\n",
        "        ultimos_registros[col] = np.nan\n",
        "ultimos_registros[\"casos_lag_1\"] = ultimos_registros[\"casos_lag_1\"].fillna(ultimos_registros.get(\"casos\", 0))\n",
        "ultimos_registros[\"casos_lag_2\"] = ultimos_registros[\"casos_lag_2\"].fillna(ultimos_registros[\"casos_lag_1\"])\n",
        "ultimos_registros[\"casos_lag_3\"] = ultimos_registros[\"casos_lag_3\"].fillna(ultimos_registros[\"casos_lag_2\"])\n",
        "ultimos_registros[\"casos_media_3\"] = ultimos_registros[\"casos_media_3\"].fillna(\n",
        "    ultimos_registros[[\"casos_lag_1\",\"casos_lag_2\",\"casos_lag_3\"]].mean(axis=1)\n",
        ")\n",
        "\n",
        "# ===== Previsão de N_FUTURO_SEMANAS (modelo escolhido — iterativa)\n",
        "previsoes_futuras = []\n",
        "\n",
        "# ordem de atualização dos lags (do maior para o menor)\n",
        "_LAG_ORDER = [12, 8, 4, 3, 2]  # lag_1 será o y_pred\n",
        "\n",
        "for _ in range(N_FUTURO_SEMANAS):\n",
        "    X_input = ultimos_registros[features].copy()\n",
        "\n",
        "    # previsão com teto FUT_CAP\n",
        "    y_pred = np.clip(modelo_escolhido.predict(X_input), 0, FUT_CAP)\n",
        "\n",
        "    # === atualiza lags: lag_k <- lag_{k-1} ; lag_1 <- y_pred ===\n",
        "    for k in _LAG_ORDER:\n",
        "        src = 1 if k == 2 else (k - 1)  # para k=2, vem de lag_1; demais, padrão k-1\n",
        "        ultimos_registros[f\"casos_lag_{k}\"] = ultimos_registros.get(f\"casos_lag_{src}\", ultimos_registros[\"casos_lag_1\"])\n",
        "\n",
        "    ultimos_registros[\"casos_lag_1\"] = y_pred\n",
        "\n",
        "    # === atualiza médias derivadas dos lags ===\n",
        "    ultimos_registros[\"casos_media_3\"] = ultimos_registros[[\"casos_lag_1\",\"casos_lag_2\",\"casos_lag_3\"]].mean(axis=1)\n",
        "    ultimos_registros[\"casos_mm3\"]     = ultimos_registros[\"casos_media_3\"]  # coerência com definição (média de 3 lags)\n",
        "\n",
        "    # avança semana ISO\n",
        "    new_years, new_weeks = advance_iso_week_vectorized(\n",
        "        ultimos_registros[\"epidemiological_year\"].values,\n",
        "        ultimos_registros[\"epidemiological_week\"].values\n",
        "    )\n",
        "    ultimos_registros[\"epidemiological_year\"] = new_years.values\n",
        "    ultimos_registros[\"epidemiological_week\"] = new_weeks.values\n",
        "\n",
        "    snap = ultimos_registros.copy()\n",
        "    snap[\"casos_previstos\"] = y_pred\n",
        "    previsoes_futuras.append(snap)\n",
        "\n",
        "df_previsoes_futuras = pd.concat(previsoes_futuras, ignore_index=True).assign(\n",
        "    casos_previstos=lambda d: d[\"casos_previstos\"].astype(float)\n",
        ")\n",
        "\n",
        "# === Suavização das previsões semanais (EMA) ===\n",
        "def suavizar_previsoes(df, col=\"casos_previstos\", alpha=0.35):\n",
        "    \"\"\"Aplica suavização exponencial progressiva ao horizonte futuro.\"\"\"\n",
        "    vals = df[col].values.astype(float)\n",
        "    out = np.zeros_like(vals)\n",
        "    out[0] = vals[0]\n",
        "    for i in range(1, len(vals)):\n",
        "        out[i] = alpha * vals[i] + (1 - alpha) * out[i-1]\n",
        "    df[col] = out\n",
        "    return df\n",
        "\n",
        "# Aplica suavização individual por UF (ou IBGE)\n",
        "previsoes_futuras_suave = (\n",
        "    df_previsoes_futuras\n",
        "    .groupby(\"codigo_ibge\", group_keys=False)\n",
        "    .apply(lambda g: suavizar_previsoes(g.sort_values([\"epidemiological_year\",\"epidemiological_week\"]),\n",
        "                                        col=\"casos_previstos\", alpha=0.45))\n",
        ")\n",
        "\n",
        "# ===== Treina LSTM e prevê N_FUTURO_SEMANAS\n",
        "res_lstm, model_lstm, scaler_lstm, lookback = treinar_lstm_separado(\n",
        "    df_feat_sem_outliers, features,\n",
        "    lookback=12, horizon=1, cutoff_test=(2025,1),\n",
        "    epochs=120, batch_size=256, lr=1e-3, verbose=1\n",
        ")\n",
        "df_previsoes_futuras_lstm = prever_n_semanas_lstm(\n",
        "    df_feat_sem_outliers, features, model_lstm, scaler_lstm, lookback,\n",
        "    n_weeks=N_FUTURO_SEMANAS\n",
        ")\n",
        "\n",
        "# ===== Agregação mensal (reais e previsões)\n",
        "# Reais\n",
        "real_m = df_feat_sem_outliers.copy()\n",
        "real_m[\"mes_dt\"] = real_m.apply(lambda r: yw_to_month_start(r[\"epidemiological_year\"], r[\"epidemiological_week\"]), axis=1)\n",
        "real_m = real_m.groupby(\"mes_dt\")[\"casos\"].sum().reset_index().rename(columns={\"casos\":\"Casos Reais\"})\n",
        "\n",
        "# Clássico escolhido\n",
        "xgb_m = df_previsoes_futuras.copy()\n",
        "xgb_m[\"mes_dt\"] = xgb_m.apply(lambda r: yw_to_month_start(r[\"epidemiological_year\"], r[\"epidemiological_week\"]), axis=1)\n",
        "xgb_m = xgb_m.groupby(\"mes_dt\")[\"casos_previstos\"].sum().reset_index().rename(columns={\"casos_previstos\":f\"Previsão {modelo_escolhido_nome}\"})\n",
        "\n",
        "# LSTM\n",
        "lstm_m = df_previsoes_futuras_lstm.copy()\n",
        "lstm_m[\"mes_dt\"] = lstm_m.apply(lambda r: yw_to_month_start(r[\"epidemiological_year\"], r[\"epidemiological_week\"]), axis=1)\n",
        "lstm_m = lstm_m.groupby(\"mes_dt\")[\"casos_previstos_lstm\"].sum().reset_index().rename(columns={\"casos_previstos_lstm\":\"Previsão LSTM\"})\n",
        "\n",
        "# Unifica e ordena\n",
        "df_month = (real_m.merge(xgb_m, on=\"mes_dt\", how=\"outer\")\n",
        "                  .merge(lstm_m, on=\"mes_dt\", how=\"outer\")\n",
        "                  .sort_values(\"mes_dt\"))\n",
        "\n",
        "ultimo_mes_real = real_m[\"mes_dt\"].max() if len(real_m) else None\n",
        "\n",
        "# === Trilhos sazonais por mês (blend com mediana histórica + teto p90*1.10) ===\n",
        "if len(real_m):\n",
        "    real_hist = real_m[real_m[\"mes_dt\"] <= ultimo_mes_real].copy() if ultimo_mes_real is not None else real_m.copy()\n",
        "    real_hist[\"mes_num\"] = pd.to_datetime(real_hist[\"mes_dt\"]).dt.month\n",
        "    saz = (real_hist\n",
        "           .groupby(\"mes_num\")[\"Casos Reais\"]\n",
        "           .agg(mediana=\"median\", p90=lambda s: np.percentile(s, 90))\n",
        "           .reset_index())\n",
        "    saz_map = {int(r.mes_num): (float(r.mediana), float(r.p90) * 1.10)\n",
        "               for r in saz.itertuples(index=False)}\n",
        "\n",
        "    def aplica_trilhos(col):\n",
        "        \"\"\"Faz blend com a mediana do mês e aplica teto sazonal p90*1.10.\"\"\"\n",
        "        if col not in df_month.columns:\n",
        "            return\n",
        "        mask_fut = df_month[\"mes_dt\"] > ultimo_mes_real if ultimo_mes_real is not None else df_month[\"mes_dt\"] >= df_month[\"mes_dt\"].min()\n",
        "        if not mask_fut.any():\n",
        "            return\n",
        "\n",
        "        meses = pd.to_datetime(df_month.loc[mask_fut, \"mes_dt\"]).dt.month\n",
        "        preds = df_month.loc[mask_fut, col].astype(float).values\n",
        "\n",
        "        alpha = 0.85  # peso da previsão no blend (ajuste conforme calibragem desejada)\n",
        "        blends = []\n",
        "        for val, m in zip(preds, meses):\n",
        "            med, cap = saz_map.get(int(m), (np.nan, np.nan))\n",
        "            if not np.isfinite(med):\n",
        "                med = np.nanmedian(preds)\n",
        "            if not np.isfinite(cap):\n",
        "                cap = np.nanpercentile(preds, 90) * 1.10\n",
        "            b = alpha * val + (1 - alpha) * med   # aproxima da mediana sazonal\n",
        "            b = min(b, cap)                        # teto sazonal\n",
        "            blends.append(b)\n",
        "\n",
        "        df_month.loc[mask_fut, col] = blends\n",
        "\n",
        "    aplica_trilhos(f\"Previsão {modelo_escolhido_nome}\")\n",
        "    aplica_trilhos(\"Previsão LSTM\")\n",
        "\n",
        "# === Suavização EMA no trecho futuro (deixa horizonte mais uniforme)\n",
        "def _ema(series, alpha=0.35):\n",
        "    vals = series.values.astype(float)\n",
        "    if len(vals) == 0: return series\n",
        "    out = np.empty_like(vals, dtype=float)\n",
        "    out[0] = vals[0]\n",
        "    for i in range(1, len(vals)):\n",
        "        out[i] = alpha*vals[i] + (1-alpha)*out[i-1]\n",
        "    return pd.Series(out, index=series.index)\n",
        "\n",
        "if ultimo_mes_real is not None:\n",
        "    mask_fut = df_month[\"mes_dt\"] > ultimo_mes_real\n",
        "    for col in [f\"Previsão {modelo_escolhido_nome}\", \"Previsão LSTM\"]:\n",
        "        if col in df_month.columns:\n",
        "            df_month.loc[mask_fut, col] = _ema(\n",
        "                df_month.loc[mask_fut, col].ffill().bfill()\n",
        "            )\n",
        "\n",
        "# === PRINT: métricas por modelo (validação temporal) ===\n",
        "print(\"\\n=== Métricas por modelo (validação temporal) ===\")\n",
        "df_scores = (\n",
        "    pd.DataFrame(\n",
        "        [(nome, met[0], met[1]) for nome, met in resultados.items()],\n",
        "        columns=[\"Modelo\", \"MSE\", \"R2\"]\n",
        "    )\n",
        "    .sort_values(\"MSE\")\n",
        ")\n",
        "\n",
        "# formatação bonita\n",
        "with pd.option_context('display.float_format', lambda x: f'{x:,.4f}'):\n",
        "    print(df_scores.to_string(index=False))\n",
        "\n",
        "# também deixa claro o melhor pelo MSE\n",
        "melhor_nome = df_scores.iloc[0, 0]\n",
        "melhor_mse  = df_scores.iloc[0, 1]\n",
        "print(f\"\\n>>> Melhor pelo MSE: {melhor_nome} (MSE={melhor_mse:,.4f})\")\n",
        "\n",
        "\n",
        "# --- Plot final (reais + previsões mensais)\n",
        "# --- Plot final (reais + previsões mensais)\n",
        "# queremos mostrar só a partir de 2020\n",
        "# garante que ambas as colunas sejam Timestamp\n",
        "df_month[\"mes_dt\"] = pd.to_datetime(df_month[\"mes_dt\"], errors=\"coerce\")\n",
        "cutoff_dt = pd.Timestamp(\"2025-01-01\")\n",
        "\n",
        "# agora a comparação funciona\n",
        "df_plot = df_month[df_month[\"mes_dt\"] >= cutoff_dt].copy()\n",
        "\n",
        "\n",
        "df_real = df_plot[[\"mes_dt\", \"Casos Reais\"]].dropna()\n",
        "df_xgb  = df_plot[[\"mes_dt\", f\"Previsão {modelo_escolhido_nome}\"]].dropna()\n",
        "df_lstm = df_plot[[\"mes_dt\", \"Previsão LSTM\"]].dropna()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# Casos Reais\n",
        "if not df_real.empty:\n",
        "    ax.plot(\n",
        "        df_real[\"mes_dt\"], df_real[\"Casos Reais\"],\n",
        "        label=\"Casos Reais\",\n",
        "        linewidth=0.9, alpha=0.35, linestyle=\"-\", zorder=1\n",
        "    )\n",
        "\n",
        "# Previsão modelo clássico\n",
        "if not df_xgb.empty:\n",
        "    ax.plot(\n",
        "        df_xgb[\"mes_dt\"], df_xgb[f\"Previsão {modelo_escolhido_nome}\"],\n",
        "        label=f\"Previsão {modelo_escolhido_nome}\",\n",
        "        linewidth=1.0, linestyle=\"--\", marker=\"o\", markersize=4, markevery=2,\n",
        "        zorder=5\n",
        "    )\n",
        "\n",
        "# Previsão LSTM\n",
        "if not df_lstm.empty:\n",
        "    ax.plot(\n",
        "        df_lstm[\"mes_dt\"], df_lstm[\"Previsão LSTM\"],\n",
        "        label=\"Previsão LSTM\",\n",
        "        linewidth=1.0, linestyle=\":\", marker=\"s\", markersize=4, markevery=2,\n",
        "        zorder=6\n",
        "    )\n",
        "\n",
        "# (mantém o cálculo do ultimo_mes_real como está, não precisa mexer)\n",
        "\n",
        "# Demarcação do futuro em relação ao último mês observado\n",
        "if ultimo_mes_real is not None:\n",
        "    x0 = ultimo_mes_real + timedelta(days=1)\n",
        "    ax.axvline(x0, linestyle=\"--\", alpha=0.6, zorder=2)\n",
        "    xmax = df_plot[\"mes_dt\"].max()\n",
        "    if xmax is not None and xmax > x0:\n",
        "        ax.axvspan(x0, xmax, color=\"gray\", alpha=0.08, zorder=0)\n",
        "\n",
        "ax.set_title(f\"Casos de Dengue por Mês — Reais e Previsões (próximas {N_FUTURO_SEMANAS} semanas)\")\n",
        "ax.set_xlabel(\"Mês\"); ax.set_ylabel(\"Casos (soma no mês)\")\n",
        "ax.grid(True, axis='y', linestyle='--', alpha=0.5)\n",
        "\n",
        "# Eixo X mensal legível (agora usando df_plot)\n",
        "if len(df_plot):\n",
        "    ax.xaxis.set_major_locator(mdates.MonthLocator(interval=max(1, len(df_plot)//12)))\n",
        "    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m\"))\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "handles, labels = ax.get_legend_handles_labels()\n",
        "order = [1, 2, 0]\n",
        "ax.legend([handles[i] for i in order], [labels[i] for i in order], loc=\"upper left\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# ==========PLOTAGEM GRÁFICO 2 E 3 =================================#\n",
        "# ====== AJUDARES P/ DATAS (semana ISO -> data-início) ======\n",
        "def week_start_date(y, w):\n",
        "    y, w = int(y), int(w)\n",
        "    try:\n",
        "        return date.fromisocalendar(y, w, 1)\n",
        "    except ValueError:\n",
        "        return date.fromisocalendar(y, 52, 1)\n",
        "\n",
        "def add_week_start_col(df, y_col=\"epidemiological_year\", w_col=\"epidemiological_week\", out_col=\"week_dt\"):\n",
        "    df = df.copy()\n",
        "    df[out_col] = [week_start_date(y, w) for y, w in zip(df[y_col], df[w_col])]\n",
        "    return df\n",
        "\n",
        "# identificar coluna de UF (se existir), senão usar codigo_ibge\n",
        "UF_COL = \"uf\" if \"uf\" in df_feat_sem_outliers.columns else (\"UF\" if \"UF\" in df_feat_sem_outliers.columns else None)\n",
        "ID_COL = UF_COL if UF_COL is not None else \"codigo_ibge\"\n",
        "\n",
        "# ====== DATAFRAMES SEMANAIS (histórico e previsões) ======\n",
        "hist_w = df_feat_sem_outliers[[ID_COL, \"epidemiological_year\", \"epidemiological_week\", \"casos\"]].copy()\n",
        "hist_w = add_week_start_col(hist_w, out_col=\"week_dt\")\n",
        "\n",
        "pred_xgb_w = df_previsoes_futuras[[ID_COL, \"epidemiological_year\", \"epidemiological_week\", \"casos_previstos\"]].copy()\n",
        "pred_xgb_w = add_week_start_col(pred_xgb_w, out_col=\"week_dt\").rename(columns={\"casos_previstos\": \"casos_previstos_xgb\"})\n",
        "\n",
        "pred_lstm_w = df_previsoes_futuras_lstm[[ID_COL, \"epidemiological_year\", \"epidemiological_week\", \"casos_previstos_lstm\"]].copy()\n",
        "pred_lstm_w = add_week_start_col(pred_lstm_w, out_col=\"week_dt\")\n",
        "\n",
        "# última semana com dado real (para traçar a linha vertical no gráfico)\n",
        "ultimo_dia_real = hist_w[\"week_dt\"].max() if len(hist_w) else None\n",
        "\n",
        "# ===== 1) LINHA SEMANAL POR UF (1 FIGURA POR UF) =====\n",
        "# cria pasta de saída\n",
        "os.makedirs(\"fig_semanais_por_uf\", exist_ok=True)\n",
        "\n",
        "for gid, g_real in hist_w.sort_values([\"week_dt\"]).groupby(ID_COL):\n",
        "    # recortes por UF\n",
        "    g_xgb  = pred_xgb_w[pred_xgb_w[ID_COL] == gid].sort_values(\"week_dt\")\n",
        "    g_lstm = pred_lstm_w[pred_lstm_w[ID_COL] == gid].sort_values(\"week_dt\")\n",
        "\n",
        "    # se não houver previsão, pula\n",
        "    if g_xgb.empty and g_lstm.empty:\n",
        "        continue\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 5))\n",
        "\n",
        "    # série real (fina e discreta)\n",
        "    if not g_real.empty:\n",
        "        ax.plot(g_real[\"week_dt\"], g_real[\"casos\"], label=\"Reais (semana)\", linewidth=0.8, alpha=0.45, color=\"black\")\n",
        "\n",
        "    # previsão XGB (se existir)\n",
        "    if not g_xgb.empty:\n",
        "        ax.plot(g_xgb[\"week_dt\"], g_xgb[\"casos_previstos_xgb\"],\n",
        "                label=\"Prev. Modelo Escolhido (semana)\",\n",
        "                linewidth=1.6, linestyle=\"-\", marker=None)\n",
        "\n",
        "    # previsão LSTM (se existir)\n",
        "    if not g_lstm.empty:\n",
        "        ax.plot(g_lstm[\"week_dt\"], g_lstm[\"casos_previstos_lstm\"],\n",
        "                label=\"Prev. LSTM (semana)\",\n",
        "                linewidth=1.2, linestyle=\"--\", marker=None)\n",
        "\n",
        "    # faixa do futuro\n",
        "    if ultimo_dia_real is not None:\n",
        "        x0 = ultimo_dia_real + timedelta(days=1)\n",
        "        ax.axvline(x0, linestyle=\"--\", alpha=0.5)\n",
        "        x_max = max(\n",
        "            [d for d in [g_real[\"week_dt\"].max() if not g_real.empty else None,\n",
        "                         g_xgb[\"week_dt\"].max() if not g_xgb.empty else None,\n",
        "                         g_lstm[\"week_dt\"].max() if not g_lstm.empty else None]\n",
        "             if d is not None]\n",
        "        )\n",
        "        if x_max > x0:\n",
        "            ax.axvspan(x0, x_max, color=\"gray\", alpha=0.08)\n",
        "\n",
        "    titulo = f\"UF: {gid}\" if UF_COL else f\"IBGE: {gid}\"\n",
        "    ax.set_title(f\"Casos de Dengue — Série Semanal (Histórico e Previsões) — {titulo}\")\n",
        "    ax.set_xlabel(\"Semana (início)\"); ax.set_ylabel(\"Casos na semana\")\n",
        "    ax.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
        "    ax.legend(loc=\"upper left\")\n",
        "    plt.tight_layout()\n",
        "    # salva arquivo por UF\n",
        "    safe_gid = str(gid).replace(\"/\", \"_\")\n",
        "    plt.savefig(f\"fig_semanais_por_uf/serie_semanal_{safe_gid}.png\", dpi=150)\n",
        "    plt.close(fig)\n",
        "\n",
        "print(\"Gráficos semanais por UF salvos em: fig_semanais_por_uf/*.png\")\n",
        "\n",
        "# ===== 2) LINHA SEMANAL AGREGADA (BRASIL) =====\n",
        "hist_br = hist_w.groupby(\"week_dt\", as_index=False)[\"casos\"].sum()\n",
        "pred_xgb_br = pred_xgb_w.groupby(\"week_dt\", as_index=False)[\"casos_previstos_xgb\"].sum()\n",
        "pred_lstm_br = pred_lstm_w.groupby(\"week_dt\", as_index=False)[\"casos_previstos_lstm\"].sum()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# série real (fina e discreta)\n",
        "if not hist_br.empty:\n",
        "    ax.plot(hist_br[\"week_dt\"], hist_br[\"casos\"], label=\"Reais (Brasil)\", linewidth=0.8, alpha=0.45, color=\"black\")\n",
        "\n",
        "# previsão XGB (se existir)\n",
        "if not pred_xgb_br.empty:\n",
        "    ax.plot(pred_xgb_br[\"week_dt\"], pred_xgb_br[\"casos_previstos_xgb\"],\n",
        "            label=\"Prev. Modelo Escolhido (Brasil)\",\n",
        "            linewidth=1.6, linestyle=\"-\", marker=None)\n",
        "\n",
        "# previsão LSTM (se existir)\n",
        "if not pred_lstm_br.empty:\n",
        "    ax.plot(pred_lstm_br[\"week_dt\"], pred_lstm_br[\"casos_previstos_lstm\"],\n",
        "            label=\"Prev. LSTM (Brasil)\",\n",
        "            linewidth=1.2, linestyle=\"--\", marker=None)\n",
        "\n",
        "# faixa do futuro\n",
        "if ultimo_dia_real is not None:\n",
        "    x0 = ultimo_dia_real + timedelta(days=1)\n",
        "    ax.axvline(x0, linestyle=\"--\", alpha=0.5)\n",
        "    x_max = max(\n",
        "        [d for d in [hist_br[\"week_dt\"].max() if not hist_br.empty else None,\n",
        "                     pred_xgb_br[\"week_dt\"].max() if not pred_xgb_br.empty else None,\n",
        "                     pred_lstm_br[\"week_dt\"].max() if not pred_lstm_br.empty else None]\n",
        "         if d is not None]\n",
        "    )\n",
        "    if x_max > x0:\n",
        "        ax.axvspan(x0, x_max, color=\"gray\", alpha=0.08)\n",
        "\n",
        "ax.set_title(\"Casos de Dengue — Série Semanal (Histórico e Previsões) — Brasil Agregado\")\n",
        "ax.set_xlabel(\"Semana (início)\"); ax.set_ylabel(\"Casos na semana\")\n",
        "ax.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
        "ax.legend(loc=\"upper left\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"serie_semanal_brasil.png\", dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# ===== GRÁFICO DA JANELA DE VALIDAÇÃO (MESMO CONJUNTO DAS MÉTRICAS) =====\n",
        "VAL_WEEKS = 8  # use o mesmo val_weeks usado em treinar_modelos\n",
        "\n",
        "ords = np.sort(np.unique(ord_vec))\n",
        "val_ords = ords[-VAL_WEEKS:]\n",
        "mask_val = np.isin(ord_vec, val_ords)\n",
        "\n",
        "X_val = X[mask_val]\n",
        "y_val = y[mask_val]\n",
        "y_hat_val = modelo_escolhido.predict(X_val)\n",
        "\n",
        "# monta df da janela de validação\n",
        "df_val = df_feat_sem_outliers.loc[mask_val, [\n",
        "    \"epidemiological_year\",\n",
        "    \"epidemiological_week\",\n",
        "    \"casos\"\n",
        "]].copy()\n",
        "\n",
        "def week_start_date(y, w):\n",
        "    y, w = int(y), int(w)\n",
        "    try:\n",
        "        return date.fromisocalendar(y, w, 1)\n",
        "    except ValueError:\n",
        "        return date.fromisocalendar(y, 52, 1)\n",
        "\n",
        "df_val[\"week_dt\"] = [\n",
        "    week_start_date(y, w)\n",
        "    for y, w in zip(df_val[\"epidemiological_year\"], df_val[\"epidemiological_week\"])\n",
        "]\n",
        "df_val[\"y_hat\"] = y_hat_val\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(df_val[\"week_dt\"], df_val[\"casos\"], marker=\"o\", label=\"Reais (validação)\")\n",
        "plt.plot(df_val[\"week_dt\"], df_val[\"y_hat\"], marker=\"s\", linestyle=\"--\",\n",
        "         label=f\"Previsto - {modelo_escolhido_nome}\")\n",
        "plt.title(\"Janela de Validação Temporal — Real vs Previsto\")\n",
        "plt.xlabel(\"Semana\")\n",
        "plt.ylabel(\"Casos\")\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l41v0C5RUhGx",
        "outputId": "9744a20a-c565-4d2b-e521-331f8603553d",
        "collapsed": true
      },
      "id": "l41v0C5RUhGx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Dados coletados para AC (2010-2025)\n",
            "✓ Dados coletados para AL (2010-2025)\n",
            "✓ Dados coletados para AM (2010-2025)\n",
            "✓ Dados coletados para AP (2010-2025)\n",
            "✓ Dados coletados para BA (2010-2025)\n",
            "✓ Dados coletados para CE (2010-2025)\n",
            "✓ Dados coletados para DF (2010-2025)\n",
            "✓ Dados coletados para ES (2010-2025)\n",
            "✓ Dados coletados para GO (2010-2025)\n",
            "✓ Dados coletados para MA (2010-2025)\n",
            "✓ Dados coletados para MG (2010-2025)\n",
            "✓ Dados coletados para MS (2010-2025)\n",
            "✓ Dados coletados para MT (2010-2025)\n",
            "✓ Dados coletados para PA (2010-2025)\n",
            "✓ Dados coletados para PB (2010-2025)\n",
            "✓ Dados coletados para PE (2010-2025)\n",
            "✓ Dados coletados para PI (2010-2025)\n",
            "✓ Dados coletados para PR (2010-2025)\n",
            "✓ Dados coletados para RJ (2010-2025)\n",
            "✓ Dados coletados para RN (2010-2025)\n",
            "✓ Dados coletados para RO (2010-2025)\n",
            "✓ Dados coletados para RR (2010-2025)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 3) HEATMAP FUTURO (semanas futuras x UF) — intensidade prevista =====\n",
        "if not pred_xgb_w.empty:\n",
        "    # recorta somente semanas posteriores ao último real\n",
        "    corte = ultimo_dia_real if ultimo_dia_real else pred_xgb_w[\"week_dt\"].min()\n",
        "    fut_mask = pred_xgb_w[\"week_dt\"] > corte\n",
        "\n",
        "    heat = pred_xgb_w.loc[fut_mask, [ID_COL, \"week_dt\", \"casos_previstos_xgb\"]].copy()\n",
        "\n",
        "    if heat.empty:\n",
        "        print(\"Nenhuma semana futura encontrada para o heatmap.\")\n",
        "    else:\n",
        "        # pivot UF x semana\n",
        "        heat_pv = heat.pivot_table(\n",
        "            index=ID_COL,\n",
        "            columns=\"week_dt\",\n",
        "            values=\"casos_previstos_xgb\",\n",
        "            aggfunc=\"sum\"\n",
        "        ).fillna(0)\n",
        "\n",
        "        # mapeamento de códigos IBGE -> sigla UF\n",
        "        mapa_uf = {\n",
        "            1100205: \"RO\", 1200401: \"AC\", 1302603: \"AM\", 1400100: \"RR\", 1501402: \"PA\", 1600303: \"AP\",\n",
        "            1702109: \"TO\", 2111300: \"MA\", 2201101: \"PI\", 2304400: \"CE\", 2408102: \"RN\", 2507507: \"PB\",\n",
        "            2611606: \"PE\", 2704302: \"AL\", 2800308: \"SE\", 2927408: \"BA\", 3106200: \"MG\", 3205309: \"ES\",\n",
        "            3304557: \"RJ\", 3509502: \"SP\", 4106902: \"PR\", 4205407: \"SC\", 4314902: \"RS\", 5002704: \"MS\",\n",
        "            5103403: \"MT\", 5208707: \"GO\", 5300108: \"DF\"\n",
        "        }\n",
        "\n",
        "        # troca ID (código) por sigla da UF, se existir\n",
        "        heat_pv.index = [mapa_uf.get(int(c), c) for c in heat_pv.index]\n",
        "\n",
        "        # ordena UFs alfabeticamente\n",
        "        heat_pv = heat_pv.sort_index()\n",
        "\n",
        "        # formata colunas (datas) como string legível\n",
        "        heat_pv.columns = [c.strftime(\"%Y-%m-%d\") for c in heat_pv.columns]\n",
        "\n",
        "        # gera figura com tamanho adaptado\n",
        "        plt.figure(\n",
        "            figsize=(\n",
        "                min(14, 2 + 0.18 * heat_pv.shape[1]),\n",
        "                0.45 * heat_pv.shape[0] + 2\n",
        "            )\n",
        "        )\n",
        "\n",
        "        sns.heatmap(\n",
        "            heat_pv,\n",
        "            cmap=\"YlOrRd\",\n",
        "            cbar_kws={\"label\": \"Casos previstos (semana)\"}\n",
        "        )\n",
        "\n",
        "        plt.title(\"Mapa de Calor — Previsões Semanais por UF (Futuro)\")\n",
        "        plt.xlabel(\"Semana (início)\")\n",
        "        plt.ylabel(\"UF\")\n",
        "        plt.xticks(rotation=45, ha=\"right\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(\"heatmap_previsao_semana_por_uf.png\", dpi=150)\n",
        "        plt.show()\n",
        "else:\n",
        "    print(\"pred_xgb_w está vazio — não foi possível gerar o heatmap.\")\n"
      ],
      "metadata": {
        "id": "TEO4vhbwi6Uv"
      },
      "id": "TEO4vhbwi6Uv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import date, timedelta\n",
        "import matplotlib.dates as mdates\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# ========= GRÁFICO DE VALIDAÇÃO =========\n",
        "VAL_WEEKS = 8  # use o mesmo val_weeks usado em treinar_modelos\n",
        "\n",
        "ords = np.sort(np.unique(ord_vec))\n",
        "val_ords = ords[-VAL_WEEKS:]\n",
        "mask_val = np.isin(ord_vec, val_ords)\n",
        "\n",
        "X_val = X[mask_val]\n",
        "y_val = y[mask_val]\n",
        "y_hat_val = modelo_escolhido.predict(X_val)\n",
        "\n",
        "df_val = df_feat_sem_outliers.loc[mask_val, [\n",
        "    \"epidemiological_year\",\n",
        "    \"epidemiological_week\",\n",
        "    \"casos\"\n",
        "]].copy()\n",
        "\n",
        "def week_start_date(y, w):\n",
        "    y, w = int(y), int(w)\n",
        "    try:\n",
        "        return date.fromisocalendar(y, w, 1)\n",
        "    except ValueError:\n",
        "        return date.fromisocalendar(y, 52, 1)\n",
        "\n",
        "# converte ano/semana em data e adiciona previsões\n",
        "df_val[\"week_dt\"] = [week_start_date(y, w) for y, w in zip(df_val[\"epidemiological_year\"], df_val[\"epidemiological_week\"])]\n",
        "df_val[\"y_hat\"] = y_hat_val\n",
        "\n",
        "# AGREGAÇÃO BRASIL (soma total de casos por semana)\n",
        "df_val_br = (\n",
        "    df_val\n",
        "    .groupby(\"week_dt\", as_index=False)\n",
        "    .agg({\n",
        "        \"casos\": \"sum\",\n",
        "        \"y_hat\": \"sum\"\n",
        "    })\n",
        ")\n",
        "\n",
        "# ========= GRÁFICO DE FUTURO (já vem do pipeline) =========\n",
        "df_plot_future = df_month[df_month[\"mes_dt\"] >= pd.Timestamp(\"2025-01-01\")].copy()\n",
        "\n",
        "# ========= FIGURA COM DOIS SUBPLOTS =========\n",
        "fig, axes = plt.subplots(2, 1, figsize=(12, 10), sharex=False)\n",
        "\n",
        "# --- [1] VALIDAÇÃO AGREGADA (BRASIL) ---\n",
        "ax1 = axes[0]\n",
        "ax1.plot(\n",
        "    df_val_br[\"week_dt\"], df_val_br[\"casos\"],\n",
        "    marker=\"o\", linestyle=\"-\", label=\"Reais (Validação - Brasil)\",\n",
        "    color=\"black\", alpha=0.7\n",
        ")\n",
        "ax1.plot(\n",
        "    df_val_br[\"week_dt\"], df_val_br[\"y_hat\"],\n",
        "    marker=\"s\", linestyle=\"--\", label=f\"Previsto ({modelo_escolhido_nome} - Brasil)\",\n",
        "    color=\"tab:orange\", alpha=0.9\n",
        ")\n",
        "ax1.set_title(\" Janela de Validação Temporal — Brasil Agregado (Real vs Previsto)\")\n",
        "ax1.set_xlabel(\"Semana (início)\")\n",
        "ax1.set_ylabel(\"Casos (soma das capitais)\")\n",
        "ax1.grid(True, linestyle=\"--\", alpha=0.5)\n",
        "ax1.legend(loc=\"upper left\")\n",
        "\n",
        "# --- [2] FUTURO ---\n",
        "ax2 = axes[1]\n",
        "if \"Casos Reais\" in df_plot_future.columns:\n",
        "    ax2.plot(df_plot_future[\"mes_dt\"], df_plot_future[\"Casos Reais\"],\n",
        "             label=\"Casos Reais\", linewidth=0.9, alpha=0.3, color=\"black\")\n",
        "if f\"Previsão {modelo_escolhido_nome}\" in df_plot_future.columns:\n",
        "    ax2.plot(df_plot_future[\"mes_dt\"], df_plot_future[f\"Previsão {modelo_escolhido_nome}\"],\n",
        "             label=f\"Prev. {modelo_escolhido_nome}\", linestyle=\"--\", marker=\"o\")\n",
        "if \"Previsão LSTM\" in df_plot_future.columns:\n",
        "    ax2.plot(df_plot_future[\"mes_dt\"], df_plot_future[\"Previsão LSTM\"],\n",
        "             label=\"Prev. LSTM\", linestyle=\":\", marker=\"s\")\n",
        "\n",
        "# marca o corte do último mês real\n",
        "ultimo_mes_real = real_m[\"mes_dt\"].max() if len(real_m) else None\n",
        "if ultimo_mes_real is not None:\n",
        "    ax2.axvline(ultimo_mes_real + timedelta(days=1), linestyle=\"--\", alpha=0.5, color=\"gray\")\n",
        "    xmax = df_plot_future[\"mes_dt\"].max()\n",
        "    if xmax > ultimo_mes_real:\n",
        "        ax2.axvspan(ultimo_mes_real, xmax, color=\"gray\", alpha=0.08)\n",
        "\n",
        "ax2.set_title(f\" Previsão de Casos Futuros — Próximas {N_FUTURO_SEMANAS} Semanas\")\n",
        "ax2.set_xlabel(\"Mês\")\n",
        "ax2.set_ylabel(\"Casos\")\n",
        "ax2.grid(True, linestyle=\"--\", alpha=0.5)\n",
        "ax2.legend(loc=\"upper left\")\n",
        "\n",
        "# --- formato de datas ---\n",
        "ax2.xaxis.set_major_locator(mdates.MonthLocator(interval=max(1, len(df_plot_future)//12)))\n",
        "ax2.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m\"))\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0-n0eU3PCz71"
      },
      "id": "0-n0eU3PCz71",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ============================\n",
        "# Tabela de métricas por modelo\n",
        "# ============================\n",
        "\n",
        "linhas = []\n",
        "\n",
        "# 1) modelos clássicos -> resultados: {nome: (mse, r2)}\n",
        "media_y = float(np.mean(y)) if len(y) else np.nan\n",
        "\n",
        "for nome, (mse, r2) in resultados.items():\n",
        "    rmse = np.sqrt(mse)\n",
        "    nrmse = (rmse / media_y * 100) if media_y != 0 else np.nan\n",
        "    linhas.append([nome, mse, rmse, r2, nrmse])\n",
        "\n",
        "# 2) (Opcional) incluir LSTM, se res_lstm existir\n",
        "# res_lstm: {\"LSTM\": (mse_te, rmse_te, r2_te, nrmse_te)}\n",
        "if \"res_lstm\" in globals():\n",
        "    for nome, (mse, rmse, r2, nrmse) in res_lstm.items():\n",
        "        linhas.append([nome, mse, rmse, r2, nrmse])\n",
        "\n",
        "# Monta DataFrame\n",
        "df_metricas = pd.DataFrame(\n",
        "    linhas,\n",
        "    columns=[\"Modelo\", \"MSE\", \"RMSE\", \"R2\", \"nRMSE (%)\"]\n",
        ").sort_values(\"MSE\").reset_index(drop=True)\n",
        "\n",
        "# Exibe formatado\n",
        "with pd.option_context(\"display.float_format\", \"{:,.4f}\".format):\n",
        "    print(\"=== Tabela de Métricas (Validação Temporal) ===\")\n",
        "    print(df_metricas.to_string(index=False))\n",
        "\n",
        "# (opcional) salvar em arquivo\n",
        "# df_metricas.to_csv(\"metricas_validacao.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "# df_metricas.to_excel(\"metricas_validacao.xlsx\", index=False)\n"
      ],
      "metadata": {
        "id": "4mEGFUHFSwuQ"
      },
      "id": "4mEGFUHFSwuQ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}